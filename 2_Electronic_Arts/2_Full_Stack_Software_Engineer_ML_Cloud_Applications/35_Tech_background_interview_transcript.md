Here is the full transcript of the interview:

**Interviewee:** Aviral (Full stack software engineer)
**Interviewers:** Rhea, Blaze, and Daniel.

[00:22] **Aviral:** Hello.

[00:25] **Rhea:** How are you?

[00:26] **Aviral:** Doing good. How are you doing?

[00:28] **Rhea:** I am very good. We have a very big launch tomorrow and the energy is good today.

[00:35] **Aviral:** Oh okay, that's good. That's good to hear.

[00:38] **Rhea:** Yeah, definitely. Everything is looking good, so we're at the calm before the storm, you know?

[00:46] **Aviral:** Okay. I can imagine. Been there in that situation many, many times.

[00:52] **Rhea:** Yeah, yeah, exactly. It's a, it's a good time. I like it a lot.

[00:57] **Aviral:** I'm glad to hear because I've had mixed reviews from people about this time. I personally, I think would be in the same book as yours.

[1:07] **Rhea:** Yeah, I think whenever you're launching big things such as we're launching Battlefield 6 tomorrow, it's like, you just got to go into it expecting it to be a little chaotic and...

[1:19] **Aviral:** Naturally.

[1:20] **Rhea:** ...expect things to go wrong and that kind of thing, right? And that way your expectations are at the right level.

[1:28] **Aviral:** That's fair.

[1:30] **Rhea:** We're just waiting on two others to join us. I think. Yeah.

[1:39] **Aviral:** Oh hey, I can see the cat. Hello.

[1:42] **Rhea:** Yeah, my boy. He'll be joining us, but he will not be asking questions, I don't think.

[1:51] **Aviral:** Oh.

[1:56] **Daniel:** Are you on airplanes?

[1:57] **Daniel:** Hello.

[1:57] **Blaze:** Hello.

[2:00] **Blaze:** Did you say they're not asking questions? Is that what it was?

[2:03] **Daniel:** Why not asking questions?

[2:09] **Aviral:** Hello Daniel. Hello Blaze.

[2:11] **Blaze:** Yeah, nice to meet you.

[2:14] **Blaze:** I'm, we're both getting settled here.

[2:20] **Aviral:** All good. No rush from my end.

[2:25] **Blaze:** How's your day going? Looks like you're in Vancouver, hey?

[2:29] **Aviral:** Oh, it's nice and sunny today. Vancouver has a mind of its own. The weather can be anything any day, but it's nice and bright today, so I like it.

[2:41] **Blaze:** Yeah, I am at the Great Northern Way office today, so like right next to Emily Carr. I don't know if you've seen the one that's there off on Great Northern Way.

[2:49] **Aviral:** Oh, I haven't, but uh, yeah, go ahead.

[2:53] **Blaze:** No, that's it. That is the story.

[2:56] **Aviral:** Okay.

[2:58] **Rhea:** We can tell where you are Blaze because of the timber behind you.

[3:01] **Blaze:** Yeah, I do like this office quite a bit. It's pretty...

[3:05] **Rhea:** It is nice.

[3:09] **Daniel:** Cool. So, yeah. I guess we can kind of start.

[3:16] **Blaze:** Maybe just start by introducing ourselves to you. I'm Blaze. I'm the Director of Operations and Technology for the marketing group here at EA. So taking care of all the like operations tools and workflows that we use to deliver content through our artist teams.

[3:33] **Aviral:** Nice. Hi Blaze.

[3:34] **Daniel:** Yep. Hello. I work in Blaze's team and I'm, I'm tech lead in charge of the tools mostly in hands-on the automations and, yeah, tools in general that artists need to improve their productivity in general. As we have to deal with the game engine, typically Frostbite, we have some customized, let's say plugins or add-ons that make their work easy, easier. And we also work a lot with game capture, gameplay capture. So we also have kind of some specific tools for making cooler, uh, shots when, when capturing from the game. Right. Uh, yeah, we're part of this marketing, the big marketing team that EA has inside of this team. There's the craft group. That's the people that generate the trailers and the screenshots and all the gameplay shots that you see, come from this team. And the specific tooling that we have is what we maintain essentially, and we're growing the team. We need to start looking into some AI, machine learning solutions, stuff, which is why we are hiring for this position.

[5:04] **Aviral:** Awesome. Hi Daniel, it is good to hear learn about that. It sounds pretty cool.

[5:11] **Rhea:** Great. Yeah, and I'm Rhea. I'm a technical director of site reliability engineering. So I'm also in the marketing org, but not directly on Blaze or Daniel's team. My team is primarily operating the EA app, which is our competitor to Steam, but basically our e-commerce storefront as well as social destinations around EA titles specifically. And yeah, I'm here today to help out in terms of building out this new arm and this team from a technical perspective. So the cloud side of things and the scaling and reliability side of things.

[5:52] **Aviral:** Impressive.

[5:53] **Daniel:** Cool. So I guess, uh, it's your turn now that where you could like introduce yourself, explain a little bit of your background, expertise, uh, companies or projects that you've worked in and a little bit of all that.

[6:10] **Aviral:** Absolutely. Well, I'm Aviral Garg and I'm a full stack software engineer who's had experience in like five years experience at Amazon and then another year at T4G was a software consultancy before that. Um, and then several internships and um projects that I did during UBC, which is University of British Columbia, as most of you might know. Um, and, uh, yeah, so I, um, I I specialize in cloud native ML applications on AWS because of Amazon. And I'd say, um, at Amazon Beauty Tech, I led, I've led a team of eight engineers building AI-powered um systems that processed 40 terabytes plus data serving more than, um, north of, I think, 40 million, um, monthly customers across, uh, 15 different countries. Um, in terms of my technical foundation, um, I'd say it relies in Python and AWS serverless architecture. Uh, specifically technologies like Lambda, SageMaker, API Gateway, DynamoDB, and the list goes on. Um, but special shout out to, I'd say, um, using AWS CDK and CloudFormation for infrastructure as code, um, in pretty much every aspect and every project, um, that I've worked at. I'd say, um, what I thoroughly enjoy is working in cross-functional collaborative teams. I've worked extensively with like designers, data engineers, applied scientists, um, product managers, even technical artists at some points, um, and, um, I, I'd say I'm excited about learning more about EA because, um, with this particular role, I saw a good perfect like technical match, um, like the exact stack that I've been building with and I genuinely just enjoy working with, um, like just creative teams, um, and working backward from the real solution instead of just here's my task and let's just code that thing up, really understanding what we are solving, why we're solving. And, uh, one thing I'd hate myself if I didn't mention this is just as a kid as myself, I've been like a fan of EA, um, like EA Sports. It's in the game. Just hearing that that was just something I remember, all right, FIFA at, um, start that off and, um, some great memories there. Uh, so yeah, that was, I'd say a little bit something about myself. Um, one thing, you know what, I'd add also is like AI and ML, everything that's like we have this NLP revolution around us. Um, and I would say I'm at the bleeding edge of, um, like using those tools, applying them in my day-to-day workflow and really leveraging all its power while being very cautious about all the disadvantages that there are and going quite in depth of those things. Um, and, uh, yeah, just going on with full force with that.

[9:45] **Daniel:** Cool. That's awesome. Um, could you, uh, I don't know, explain a little bit of the projects that you've worked on? I mean, it's cool that you've been in Amazon and and working with this stack of technologies because that's, that's mostly what we are leaning towards for some solutions that are being explored here. Um, if you could give some details into solutions that you've developed, I I guess that when working in Amazon, most of the time you were maintaining things that were already built up before, I guess, or you had to design, uh, some solutions for new projects or for new problems or whatever.

[10:29] **Aviral:** Um, absolutely. That's a great question because, um, actually, I'd say to a lot of these questions, my answer would be it's a mix of everything because I've had a chance to deal with so many things at Amazon. Uh, but specifically to answer your question, um, like I've had, like one project that I'd love to discuss would be the virtual try-on project, which we had a chance to build from from scratch where I used all these technologies, but I've also had, um, projects where I've led, um, where I'm maintaining existing things. I wasn't in the AWS team where things are already there, S3, the all these different services. Um, I was in the Amazon Beauty Tech team. So let me give you a tiny bit of context on this virtual try-on project. Right now, if you were to go to the Amazon app, um, go onto some lipstick and, uh, click on this virtual try-on button, you would see your face, like the camera, uh, augmented reality open up, you'll see your face and then you can see what that lipstick looks like, right? Um, that right there was one project that I led. Um, we did have, now why did I say mix of both? It's, we did have a third party solution going on, um, that we were paying, uh, and they would essentially take the image, take the product images that we had in our Amazon catalog, extract color from it, and create 3D assets, 3D models, um, that would be given to the rendering team, the rendering microservice, and they would like in live tracking, it would render that 3D model, um, on the lips. Uh, for what we did was we wanted more control. This app was very expensive, this third party tool was expensive, expensive, it was non-extensible, like we wanted to include foundations, eyeliners, and other things as well. And the turnaround time was quite, quite a lot with like how much time it would take to generate those assets as we'd give them images in bulk. So, well, we wanted control. We wanted to create this. So I took it upon myself to like propose the project and take it all the way from like coming up with the requirements and deploying this, creating this all in house. Uh, what I'd say is, um, in this, uh, I worked firstly on getting the requirements. Okay, we want to replace this app. We want to build this in-house. Uh, what are our real requirements here? What what are our data sources? What are our different areas? Um, actually, before I go more and more detailed, is this something, am I going in the right direction in terms of what you're looking for? Like in terms of, um, like yeah,

[13:24] **Daniel:** Yeah, yeah, pretty much. I wanted to know a little bit more detail. No need to go super deep. That's fair. Maybe you can focus on, on what, if it was challenging, what was the most challenging part of that, what solutions or decisions you had to take or, or something like that.

[13:44] **Aviral:** Absolutely. One thing that I somewhat had prepared as well, and I think I was had started to go in that was more of a walk through of how I like the whole project came to fruition. But, um, yeah, I can definitely go go more on like maybe a first bird eye view of, um, the steps that we took and the challenges that there were in that.

[14:02] **Daniel:** If if the tech that you set up for all that is relevant, I mean, uh, all these AWS services in general, uh, makes sense. Like, feel free to explain a little bit of the architecture of everything. So yeah, no problem.

[14:18] **Aviral:** Hundred percent. Okay. I sat down with, um, uh, like the product requirement teams and, um, like we had our requirements all set up and the architecture, like basically what we had is we were consuming the entire Amazon product catalog and we had to create like data pipelines with where we are extracting all the filtering the, uh, all the products, getting exactly the lipstick images that we want. And in there, we would, um, extract like RGB colors and the visual properties about a lipstick. Like, um, so the architecture that we ended up using in order to all do all this, um, I had like, I created three different designs. There were one involved like ML Pigeon, um, which was something internally that we had used. Um, so in the system design review that I did with my entire team, um, the recommended one that I I provide was, uh, using AWS step functions. Um, created all of it using AWS CDK with like infrastructure as code with cloud formation. And in inside that step functions, we use lambda, Athena, SQS, SNS, DynamoDB, DynamoDB, um, um, triggers. Um, and, uh, there were a lot of like these kind of things. The whole idea was an, um, uh, an event driven serverless batch processing architecture, where it served actually both data generation time, like it it helped us during creating our ML models. So we needed to gather data, needed to do ML labeling, um, labeling, bounding boxes inside our images from which we would extract the color. And this pipeline then helped gather data and not only that, later on it helped us, um, replace that particular module, that step in the step functions where we were sending data labelers, these are the images. Please could you extract some, um, colors out of those or create bounding boxes for us. We use that data, I worked with, um, data scientists and applied scientists. We created our own machine learning models to do that very same work, extract those colors, finish types, and, uh, particles and things like that, like that are there inside the lipstick, and, um, replace that module with AWS, um, SageMaker endpoints, uh, where we did batch processing for such, um, visual property extractions. Um, yeah, so I I would say like that that would be like the one big challenge there was, um, was garbage in, garbage out. We would have a lot of, um, um, bad data coming in. Um, we would even, like not just images, but even in text, um, unicorn red, I remember that one example that someone would say, this is the color of our lipstick. How do we like make sense of that? What RGB colors do we extract? Um, another issue in images was, like you have lighting from different areas, we are going with pixels, um, there are some areas in the nail polish product image, um, that is just yellowish, but the color is actually more on the orange side, right? How do how do we tackle that? Uh, with these things, we like, I can go more into details about how we tackled this in a technical and non-technical ways as well, thinking inside the box and outside the box. Um, but these were the kind of challenges that were there in front of us and the way I did modular programming, um, the way I set up the whole design and infrastructure and implemented that, we allowed for not only gathering data but creating machine learning models, plug and play those new machine learning models that we put put in there and were able to serve those millions of customers and, um, make it extensible to not only just lipsticks, but foundation and, um, eyeliners, bronzers. I know too much about makeup than than I knew before this. And, um, yeah, like there there were many interesting, more such challenges and, um, I'm I'm just wondering which one would you be curious about or if you had any specific questions.

[18:39] **Blaze:** Um, I I mean, I have a question. So how did you, when you were figuring out, um, all the information that you were putting into the initial library that you're training the model on, like were you setting parameters for the storefront owners that they had to submit images of a certain type or were you just taking like all of the images that maybe provided from the storefront owner and then you're you're retrofitting this solution in?

[19:02] **Aviral:** I love that you brought it up because that was actually exactly what we looked into. We worked backward from what the problem is. We have some garbage data. We do need, some garbage data, there was a lot of like good data as well. So we applied the solution on both fronts. What we did is, this is the data that we have, and during only certain time of the year for some reason, um, we were allowed to make changes into the kind of data submission from the different clients, Maybelline, L'Oreal, and all these different companies. Um, the point of contacts that we had, we were only about to submit allowed, so so basically, long story short, like we were, um, what, we were going to get better images or the kind of images that we wanted that I worked with some artists and visual computer vision, uh, scientists, um, the way we wanted it, we were going to get it one year from now. In that meantime, we still wanted some good data. So we worked with both. We set up a plan to try to get early access to changing the requirements of the images they'd submit and even standardized a new way of like in text itself, if they can, we we showed them procedures of how they can themselves at source calculate the correct RGB values and provide that to us. And on our side, in that meantime, we created, um, like our own pipeline with that data and provided like amazing like 93% success results and I'll can even go into details of how we calculated the realism, uh, quantification there, but, um, and not even after we changed the standards after one year, um, we knew that not everyone is following that. That's why this pipeline remained essential and useful in the long run.

[20:44] **Daniel:** So this pipeline essentially filters out the garbage data or

[20:48] **Aviral:** So, um, yes, so it it, uh, it filters out like, first it like filters out like all the products that we don't need, like, um, like we wanted only beauty tech. And this actually served as a data lake later on for all our other recommendation beauty sister teams, recommendation front end and other teams. And but our particular pipeline, we took that data and then we organized different images, even did some classification and categorization of those images, but in short, yes, uh, it does filter out a lot of that, and then, uh, the next modules of stages, it would, um, send that for automated data labeling, get back that result, store it in like organized format, like, and we had like structured and unstructured data all put together and send events that, hey, these are the new images ready for, these are some new images for the existing products, or there are some new products, and now let's create the models and then serve them later on, um, yes.

[21:46] **Blaze:** Right. Cool. So this is the project that you're, that most proud of or interested in? Or is there any other projects on your, uh, your CV that you would, um, enjoy discussing?

[22:00] **Aviral:** Um, I would say this is, uh, amongst the the the, like the the project that I'm most proud of, but there was also like a prototype project, uh, that we, um, worked on, um, um, that would be the Project Scott, I would say. Uh, what this Project Scott was, we had a lot of complaints about documentation internally. Um, we have...

[22:24] **Blaze:** I have a lot of complaints about documentation internally.

[22:27] **Aviral:** Well, perfect, then I have the right tool for you. Um, what I'd say, uh, what we had built was a prototype of, um, Scott attached to the Slack. Let's say if you wanted to join Amazon Beauty Tech, not that I am doing that right now. But if you joined, um, the beauty tech team, you would, you could talk to Scott on Slack, send a message, you'd have a question. First thing, it would recognize which all topics that the question is related to, using NLP, LLMs. And then it would further look up the documentation, do retrieval augmented generation, RAG, um, systems using and and using vector databases, Pinecone, um, it would get back the most relevant results. If there was a partial match to the question or there was a zero match, like to the questions, what one big initiative that I'd done organization-wide was every one should own, like every piece of topic, every piece of project should have a documentation owner as well, which would be the same as the actual owner who created that project. So I had an organization-wide initiative, we had three different workshops. Everyone took ownership of we are going to make sure that our, there's 100% reference for every single topic. Any new comer who comes to the, joins the team would know, um, who to contact when, um, when you have a question about something and you're stuck at something and you couldn't find anything in the documentation. I, because I'd done that initiative, I used that into the Project Scott and where I was saying, if there was a partial match or a zero match, Scott would essentially contact those subject matter experts to ask specific smaller questions, not just the exact copy paste of what you would have asked on Slack, but only the parts that were missing in the documentation. They would respond to it. Scott would fill up that documentation or suggest filling it up because we hadn't turned on the, um, the the the the the mode to just completely autonomously like change documentation, but it would provide suggestions and it become a one click task to fill those documentation and respond back to you, here's the answer. Now, why wait for just like someone to ask questions and then we find documentation gaps. We created agents, naturally, where we would ask ourselves, become the new person joining the team, ask questions about different topics and see where there were documentation gaps. And then it would suggest, like this is how you should, um, like, yeah, do, like this this is what you should add in there. Amazingly, we had nice, I'll I'll wrap it up. Sorry, but if you have a question, please go ahead.

[25:28] **Blaze:** No, I was just going to ask, like, how is it handling legacy data or cleaning up like old archives? Cuz I think that's going to be, like, like we would love to have something like this implemented, but I I wonder like, how is it going to get rid of all the old garbage, you know?

[25:41] **Aviral:** A lot of involvement from, like a lot of red teaming, um, from, this is one of the biggest challenges that we ourselves had, but it was in very early prototype stage, so I would not say that I have all the perfect right answers, but we were facing these same issues and, um, there was a lot of red teaming done. Everything, like we, Scott was bugging a lot of developers in the early stages a lot, but based on the patterns that we saw, we we saw like there were there were like similar kind of questions being asked, similar kinds of documentation being, um, being being, um, like there being a gap of that, and similar kind of mistakes being made in there. Once, um, it had learned how to, uh, like, like how to fill those gaps or how to provide the right suggestion and those subject matter experts, we already automatically knew who to ask instead of just sending it to me as the developer, like here are all the questions, can you please clean up the data? We already had the setup for, like this, I'm going to redirect this question, is this answer correct or not, partially correct, please provide some insights into all that. And, um, yeah, that's I think how we we we tackled all of this. Um, I was the lead developer on it, um, but there were like other, um, teammates as well, but it was something because of that initiative I'd started, um, of the document ownership and trying to get our 100%, uh, documentation completion, um, it, it is, like it, we we just took it forward with using these one thing. Uh, using this Project Scott. One quick thing I'd mention is one biggest learning point from it was we saw that don't stick to creating your own machine learning models. Um, OpenAI, Anthropic, all these big companies do, they're doing an amazing job at creating these great LLMs. We either go for the local versions of these same one as we had them in bedrock, or even use simple smaller, um, models, local models like inside Olama and just ask simple questions, start from small, start basic, and then just be language agnostic, do great prompt engineering. So this was very much so I turned out to be a prompt engineering, um, an agentic handoff framework, um, project as compared to, um, creating our own machine learning model.

[28:13] **Daniel:** Interesting. Cool, cool. That's quite relevant. Uh, just kind of a side question, it's not like directly related to that. I'm curious to know if you have any experience in game related development. It's not strictly necessary, but if you're familiar with Unity or any 3D vector math, whatever, are you good with that?

[28:41] **Aviral:** Actually, um, during my, this, I would say my experience has mostly been during my university times and a personal projects later on, but if you're okay with like learning about a little bit of what I did at university, I can go ahead with that. But I do not have professional experience with it. Um, but I did lead like a team, um, at like my capstone project and the last, like in the fourth year of my university, uh, we worked with neuroscientists to have, uh, HoloLens where we used Unity to, um, like create mixed reality experience where we compared a normal brain to a brain affected with some disease and, um, like created in like front end and even the back end and even automated testing scenario for like doing smoke testings and showing all the use cases of where the person can go and click different buttons. We did that. Um, another, I don't know if you guys have, if anyone of you would have seen Sherlock, the BBC TV show. Uh, any takers on that? No?

[29:47] **Blaze:** Sherlock? No, I don't think so.

[29:49] **Aviral:** So he goes into a memory palace. So I don't know if you're aware of the memory palace technique. I I see a head nod. Um, memory palace would be you'd go inside, um, like your own palace, places that you remember, and you'd place like kind of surprising and things that would stick out inside the palace in those different areas, um, to remember things. So I created a Japanese language learning memory palace, um, in virtual reality, again, using 3D, uh, or Unity 3D, uh, where, um, the hiragana characters would be, you would place the word, a key, an actual key, I still remember you go inside and take a right turn in the palace, the first thing you'd see is the key, like an actual giant key there because the key character in Hiragana actually looks like the key. Um, and I see Rhea actually agreeing with me there. So it's it's being validated too. Um, I actually learned the first 50 elements of the periodic table as well, um, using this technique. Um, and one interesting thing that I did was I combined back end to this front end experience that I had in Unity. Um, back in the, back in the day, I'd say, um, uh, I, you could search things up dynamically and you could, like, in with Google APIs, you could search different images. Hey, I want to place an elephant right there. So, uh, you could, like, do the search while you were wearing the virtual reality headset and you could place, create a 3D model of that elephant and place it right there. And we created even like a platform for developers to create 3D models or, or submit their 3D models and it would automatically get populated in the local library. There are so many things I could talk about all that, but, um, it's it was something fun that I've experienced with with Unity 3D, uh, a few more personal projects, and that I really enjoyed.

[31:55] **Daniel:** Yeah, good enough. Good enough. Sounds interesting. Cool. Um, so, I'm pretty good with that. I don't know, Rhea, if you have some technical questions or something like that.

[32:06] **Rhea:** I do. I do have a few. Um, if it's cool to go back to the the beauty tech projects, because that intrigues me a lot. When you talk about scale, and that's my thing. So you mentioned 40 terabytes of data in your catalog per day. And you were consuming all that into your custom models. Did you run into scaling issues in any part of that pipeline in your lambdas, in your step functions?

[32:35] **Aviral:** Oh, 100%. Um, in the early, like of course. In the the early stages when I'm exploring and doing my like time boxed, um, let's see what all tools are out there. There were numerous tools in AWS services that are publicly available and even internally. There was Glue, there's data pipelines, there's like so many things out there. And a lot of them failed the scaling test. Um, we, or they passed the scaling test, but it got way too expensive. So I felt like the, like when I kept looking at this problem again and again, we would always come across this, there are always out of the box ways that you can use to like change your data source. Like, don't just try to solve this one thing with the constraints that you have. I looked around and the same data was available in different format, um, that we had, like we had like streaming data coming in, but we didn't need streaming data. That would make things with like super expensive, AWS Kinesis and whatnot. There was data coming in in different formats. The one format that worked for us was like the parquet format with Athena, where we could, using Athena, um, it would be able, we would be able to do all this ETL pipeline where we are, um, filtering the data. And that right there was kind of like a bingo moment for us because that gives us a lot of flexibility in um, how that that was one of the steps in the step function and we would use Athena to query the data, capture anything and everything that we needed. Um, and um, yeah, like so so that would be one place where I'd say we tackle the scaling challenge. Even when we were extending that to like different, multiple different countries, we had multiple data coming in, things were all nicely kept for us in S3 buckets. Um, and we were able to consume that and like Athena would allow us to do SQL queries on top of like the parquet format data in S3, uh, nicely, good, nicely like good filtered and everything, and we actually skipped a lot of complexities that we could have added, um, and brought in data engineers to do very complex SQL queries. We ended up with a lot simpler, um, yeah, like, like just because of we looked more broader and saw what all things are available and if there are simpler, changeable solutions that we could, um, add to our modular, um, serverless pipeline.

[35:17] **Rhea:** Okay. And then that sounds like mostly upfront in the technical design. Was there anything where you were 70 or 80% of the way through implementation or you were feature complete but it was not working or not scaling?

[35:34] **Aviral:** Um, I'd have to think about that. Um, because a lot of the examples that are coming to my mind right now when we were scaling to, we were extending to Japan, um, we saw that our results were absolute garbage suddenly. Um, and, uh, why is that happening? And then we looked more into it and it turned out that there were a lot of like specific, like for yellow, yellow colored lipsticks, we were getting black colored like 3D models. Um, so there were, I'm not sure if it was really a scaling problem but more like an extensibility problem. Um, yeah, uh, so there we found out like with our data and ML scientists, um, that, uh, there was some bee that would always be there in a certain brand, uh, images, like a honey bee, and the black and yellow and for some reason, that would always, uh, go more, like, like detect the black and it would do an average averaging out of, um, that. So there were very small simple use cases that messed up a lot of our data. Um, if you could give me a moment to think about a good scalable scalability issue problem that we came across that I came across. We were 70% complete and, um, maybe after launch too. I I guess I I would say I'm very happy that I can't think of anything right now because, uh, we did not have that, um, kind of issue. Let me,

[37:09] **Rhea:** So maybe I can pivot the question a little bit because it sounds like you did a lot of upfront planning, um, which is amazing. When you were looking at all these different approaches that you've mentioned, how were you designing or thinking about scalability between them? Did you have specific requirements or benchmarking that you were doing?

[37:31] **Aviral:** Absolutely. So, one thing at Amazon that we've always learned is to do operational readiness. We create our operational readiness requirements document, OR, operational readiness review before we actually start implementing. We put in place all the things that we are taking care of if things fail. If there's a day, like a, like a long failure in, um, the upstream data that we're getting it from, what is the backup mechanism there? If there are so many lambdas there, we have cold start problems. Um, how how do we take care of that? Um, if some lambda for some reason, even after tackling the cold start issue, they're they're just not responding, like do you just spam it and and like increase our cost for no reason? So all those kind of things are something we looked up front and we have like a whole big checklist of, um, do exponential back of mechanism. That was something that I introduced in our, um, our team because it was in available in, um, our, in in step function service. Um, and it was something new that had shown up like during that time when I was implementing, um, we use X-ray for tracking those bottleneck things. Anything and everything that could go wrong, we did keep that in mind without letting that slowing us down. Like obviously we didn't over engineer things, but always kept, like one biggest tenant that, uh, that we, uh, followed that I followed and I made sure all the developers under me also followed was, keep things modular in a way like we can plug and play, we can change things around, we have control over everything. And that was one of the reasons why I went, we went for the recommended design that I had offered, the design number three, I still remember, because it we didn't hand over things to a different team. We had control over every aspect. We were very mindful about the costs. We were very mindful about we will be doing a worldwide expansion project right after the current one. We only started with, um, with US marketplace. Then we went into Japan and then into Europe and then Asia, Mexico and other, Mexico was much earlier on actually. Um, so those, having those things in mind, having our ORR checklist, having clear design principles, clear guidelines and have good contracts between the different sub components that different, like because implementation time, everyone took their own sub component. Um, so we made sure that those contracts are being met and in all those contracts, we are following the best practices that where things go wrong, we we have like, do you have a checklist of like, have you checked all the marks for back of mechanisms or like bottleneck tracking? Do you have all the logging? How long would it take a developer to debug an issue, um, if something were to go wrong? Are those organized well? Is that documented well? So, um, I could go on and on about like how much we cared about that. Um, but those are the ways that we handled like systematically, making sure we have the checklist, even if checklist was missing something or not, making sure that we are tracking all that too.

[40:56] **Rhea:** That is a very good answer. Thank you so much. Daniel or Blaze, go ahead.

[41:03] **Daniel:** I think I'm pretty good and satisfied with your presentation of your background, way of thinking, and all that. I think that's pretty much what we're looking for. So, uh, I don't know if you have some other questions. Otherwise, we'll let you ask.

[41:24] **Blaze:** I mean, it's, um, probably good to also just like maybe give you a quick, um, summary of how our team is set up and then we can turn it over to you for questions. So, um, I mean, we've kind of talked about all the content that we're creating here, like the the core, um, uh, like direction of the marketing group is to create content for the entire portfolio. So we work on all the different projects here at EA. There's not a franchise that we are involved with. Um, we create all kinds of content. Uh, Daniel mentioned this earlier, screenshots, cinematic videos, uh, capture trailers, live action, everything. Um, our tech group, which Daniel and I are looking after, um, is going to be split into two functions. Uh, we are currently hiring for multiple roles, so this this org is really developing right now. Um, and, uh, we are going to be certainly figuring out some things as we go, um, but there are the tech designer group, which are more responsible for our pipeline efficiencies and making sure that our day-to-day projects are running. Um, so we have, um, like maybe a cinematic trailer happening or a capture trailer happening and any sort of technical issues the artists are running into on a day-to-day basis that first hits the tech designer group and they try and jump in and fix the issues. And then we will have our software engineering section of the, um, technical group and they are really like more forward thinking and trying to lay the ground work, um, for all the tools that all the artists will be utilizing to help create that content. And of course, they work very closely hand-in-hand. If the tech designers have an issue that is a reoccurring issue or something that's going to be, uh, you know, a long, need a long-term solution, then it's ticketed and handed over to the software engineering group. Uh, but at large, everyone is working together to try and make content more efficiently. Um, so that's kind of what it looks like. Um, and then the experiences org at large is a huge collective of, um, technical people, artists, um, other programmers, um, lots of like brand folks, publishing folks. It's just like, it's a really, really massive group that is, uh, doing everything that surrounds the games here at EA. So with that, I think we can turn it over to you for any questions you may have about the role or EA.

[43:38] **Aviral:** Absolutely. I do have one. Um, one thing I, but by the way, thank you for, uh, giving me the gist of, um, like what what you guys are doing and it sounds very interesting. Um, like seeing, looking into efficiency for, um, how how can people, if I understood this correctly, um, like how can content creators, um, get the right tools, um, like have access to the forward thinking right tools to, um, be able to create the content efficiently. And, um, if there are any continuously some issues happening and maintaining those pipelines and, um, getting a good idea of like, hey, there's this pattern that you're seeing again and again, um, this issue, how do we resolve that and make sure things are good up and running. So not, not just creating the right tools, but also maintaining and supporting them. Um, that's, that's, um, that is pretty pretty cool. That's pretty good. I like that. And, um, I myself, like, enjoy, uh, looking into efficiency and even my at personal level at at organizational level, in case you haven't already noticed. But, uh, yeah, no, I appreciate, uh, that. I do have a question about, um, this role in, uh, specifically, um, and it's actually a question for all three of you. And from your perspectives, I was wondering, what does success look like for this particular role that you're hiring, um, yeah.

[45:03] **Daniel:** Yeah, I'll, I'll start with this one. Um, in this context, what Blaze just presented that, uh, we essentially want to support our creators in different aspects and optimizing those creation tools. Um, as this org is kind of bigger, we are getting a requests about creating other tools for expanding productivity in other areas, like not just the content creation tools, but things such as production tracking, uh, document generation for collecting data from brand, legal, and making a brief of the production that is going to be created or automatically validating the renders, like the content visuals that are being generated. We may want to apply automatic validation of correct size logos, proportions, colors, things like that. And there's where we are getting this need of some, uh, more AI versed, uh, developers that can explore new ways of applying machine learning models for automating or making those processes like more powerful, let's say. And in that context is where we are looking for somebody who has experience in the kind of projects that you've been explaining, like how to apply ML models to, uh, extract a lot of existing data and generating something that is, uh, much more valuable, like generated information or targeted information for specific new goals that maybe haven't been explored and such. So, uh, trying to answer your question in a bit more specifically, success would be somebody who is autonomous because there's not a lot of experts in this area right now in the team because the current team is mostly focused on creating those tools, like automating workflows for artists essentially. Right. There's not a lot of AI GPT stuff around there because they are very specific tools that are hardly, uh, generated by AI because it's more like use this specific asset of this specific setup of this game to, uh, whatever, apply this specific animation. So AI hardly can contribute to that. I mean, we'll probably get there someday, but for now, it would be like, uh, really costly, I would say, to implement. So as we are expanding into exploring these new areas where there's more potential for AI, machine learning models, like to automate processes, we want someone that has experience and somebody who is able to autonomously look for, uh, what the user needs, understand, uh, what are those needs and what is the most optimal way, like somebody who can design the solution, um, preview the risks maybe and like, as Rhea was mentioning like, uh, foresee scalability issues and present the right solution that will adapt and provide a long-term solution for those new needs. So it's a bit hard to to put in in in small words, in in little words, I mean, I'm in three words.

[48:50] **Aviral:** Tell me about it.

[48:51] **Daniel:** Yeah, yeah. So, yeah, it's it's mostly about autonomy, knowing knowing your domain, like, uh, because you won't have a lot of people to ask about those new tools and establishing new ways of solving problems through AI, essentially. Right.

[49:15] **Blaze:** Yeah, maybe just to piggyback off of that, like looking for new ways to solve problems. Like I think I'm really looking for somebody who's going to be able to collaborate with our existing technology and artist teams and like, try and find smart ways for scalability through artificial intelligence. Like I think that's really what we're looking for in this role. Um, and yeah, Daniel's already mentioned this as well, like it is new. This is a new type of technology. Like, uh, I think that our guidance that we have currently is relatively loose and we're really just like looking for somebody who can be a thought starter with us and and help like guide the next phase of development within our organization. Like I think that's what really what good will look like. Um, so it it's, I mean, it's what the person wants to make out of it, you know? Like I think there is like so much, um, curiosity around these products right now within our board and we're we are searching for people that are just like excited about applying this kind of tech within our group and who are, um, happy to experiment and pivot with us and be the subject matter expert in the room.

[50:26] **Aviral:** Right. Lovely.

[50:28] **Daniel:** Correct.

[50:28] **Rhea:** Totally agree.

[50:30] **Aviral:** Um Rhea, would you have anything to add to that? I mean, like it did...

[50:33] **Rhea:** I think it's a slightly different spin, since I'm not directly on the team, but we are sisters in the same org. And also, um, I'm in a lot of more like EA-wide style groups. So despite Daniel and Blaze mentioning that there's not any experts existing on their teams, there are other central teams in EA that are working very heavily in the AI and ML space. And I think something that's important to me is getting that collaboration going and sharing information and curiosity across all these teams so that we can all learn from each other and harvest good ideas and, like, end up in the same direction is where I would like to see it. I saw Blaze's reaction. But it is a massive company. There's a lot of teams that work in their own silos and honestly, from being honest, one of the biggest issues from a tech perspective is different teams doing different things or the same thing without knowing that other teams exist. You probably had that from your experience at Amazon too. So, to me, someone that's really eager to teach, to share, to be involved is what I'm looking for anyway. And having a healthy community around the AI and ML space would be amazing.

[51:57] **Daniel:** Absolutely. Absolutely. I can totally subscribe to that. And I mean, the the success would be somebody who can actually interface and understand all that talent that is already working on that front, but we are not aware of because they are not part of our direct team. So yeah, like getting in touch, finding the right people, building those bridges, you know. So, so those projects can actually grow. That would be ideal. Yeah.

[52:31] **Aviral:** Okay. That that is very very helpful to learn different perspectives on this. I appreciate that. Um, I have a quick follow up. I'm not sure how much time allotment we have for this. Um,

[52:43] **Daniel:** Yeah, we still have some minutes.

[52:44] **Blaze:** Yeah, I have a call at two, but, uh, yeah.

[52:46] **Daniel:** Yeah. Tell us.

[52:49] **Aviral:** So it's related, like a follow up on the same. Um, it's, uh, sounds like a good greenfield project where like, you know, moonshot project, like different people call it different things, but, um, and the technology is exciting and it's it's so great to see how people can try to be excited and incorporate this. But at the same time, there's comes a sense of caution that what is the definition of done? Um, even if you have that, is that like, are you, do you have enough tools, um, to first even gather that data and how useful something like incorporating AI really is, right? Because many a times, you have, you're launching a game and you have like these many downloads, this is the revenue it created, you have a perfect success metric for this. Whereas when it comes to, um, like having, talking to the content creators, um, you can go with like how much are they using these tools and not going into tracking those things. I'm wondering how much access is there from like working backward from, hey, if we are aiming for this kind of success metric or this is how we are going to gauge those things. And it's a two-parter. I will like just quickly just add to this. Right now, even to figure out the pain points, do we have access to like, would you have access to, um, seeing how currently the content creators are using those things, like do we have programmatic access? I know like I could ask like a million other questions and get started on this, but, um, I'm just wondering and my, so my question being, um, is there access to seeing how the, what the whatever work that, um, the person in this position would be doing, the whatever they deliver, can the success for it be measured and can, is there access to data seeing like we'd be exploring, um, how the content creators are going to use these tools. Um, are is there metrics coming in for that so that we can define our end goal and even look at our sources of where to get started on.

[54:57] **Blaze:** Yes. All all both, I can start Daniel. Um, yeah, so most of the metrics that we're tracking, um, as far as like implementation of any of the products, at least within our group or, um, like success rate is all going to be based on like the amount of time that it takes to create a specific asset type. Like so we have, uh, standardized workback schedules for all the different assets. We're trying to accelerate all of that. So everything is going to be tied back to that. Um, and the place that we track all of that data or all that effort tracking is all in Airtable. Um, it's something that we've gotten a lot better about over the last couple of years and we continue to lean into across our board. So, um, is there perfect metrics for this? I mean, no. Like at the end of the day, this is art for commerce and like there is always a little bit of like a gray area as far as when we're trying to like match efficiencies for art. Um, but I do think we are doing a lot better job of tracking the amount of time it takes to create each our products. Um, and then as far as like seeing like how the artists are using this, like I think the team is small enough that you will like directly see that. Like they they sit right behind me. Like, you know what I mean? Like, yeah, it's very much like a collaboration space here and and most of the folks that are going to be using these tools are located in Vancouver. Of course, there are other sister studios as well, but, uh, no, I think, uh, there there will be a very direct connection to that.

[56:24] **Aviral:** Okay, that that's great. That's perfectly exactly what I was looking for. Okay, that that that does answer that question. Um, another question would be around, uh, if starting today, like what advice would you have for this particular role? It's kind of more of an open-ended question and I'm just wondering if there's some learning point specifically, you're coming in for this job, like this is definitely, please keep this one or two points in mind. I was I was just kind of curious about that.

[56:53] **Daniel:** Yeah, I would say that don't be shy and ask everybody like in all directions, find the right people and and and yeah, and find all the resources that you have available because there are many. Like, as as Rhea said, like there's a lot of teams working on related tech using a lot of AI resources, like a lot of, uh, products that are available that EA, the company is paying for for experimentation, because at this stage, the entire company is in one way or another, it's exploring what tech we have available, if we should use third party solutions or if we develop our own. So the initial phase, if starting on this job would be to explore everything so that you build your own map of what you have at your reach and then starting to design based on that. That would be like my initial advice.

[57:54] **Aviral:** Yeah. It almost sounds like, um, the the very initiatives and the workshops that I did to create that subject matter expert table, knowing who to reach out to, like would be a like a great initiative to have while also focusing on, uh, delivering our own results while, of course, exploration is good time boxed effort. Um, yeah, so it, it it does feel like something that would be right up my alley where we're looking to create something, um, out from scratch because like from all these answer I'm hearing, um, yeah, like it would be knowing what your resources are, figure out like how to leverage them in the right amount of time and work backward from what the biggest pain points are and kind of like, like explore what the right solutions are. Let's like create, um, a pipeline, let's create processes that would fix those issues while keeping scalability, extensibility issues in mind because after all, it is a big company and, um, that is something that one definitely has to be very mindful of and and and easy to lose grasp on that when trying to work on like something exciting and and from scratch because it, yeah, like we can create so many tools with cursor, when surf, um, right now, but, uh, can you take it forward and actually deploy it is the real deal.

[59:15] **Daniel:** Exactly. Yeah, and there's a lot of creativity, I would say that this is a kind of a nice point of this position is that there's a lot in front of you for for you to create, really.

[59:27] **Aviral:** Hundred percent, finding the right balance, I feel like would be a good key. And I I appreciate the the advice. Um, I almost feel like Rhea was about to say something and I just kept talking in case you had something to say, please continue.

[59:40] **Rhea:** I was going to say exactly what you were describing in terms of finding the experts, it's just probably on a bigger scale because now you're cross studio, cross teams, completely different wings of the company and, um, I won't lie, it is probably a bit intimidating, but I think it will be a lot of fun for whoever gets this role. And like Daniel said, I think it's going to be, don't be shy, find your people. Everyone is honestly usually really willing to share and, um, especially if they're working on something they're really excited about. So like just getting in there and working with them will be essential.

[1:00:25] **Aviral:** Definitely. It is 2:00 PM, but like I was wondering if Blaze, you had a quick comment to add to this, um, like about the same answer.

[1:00:32] **Blaze:** No, I was actually going to say thank you so much for meeting with us because I know you're up to run. But, um, I really enjoyed our conversation today and, uh, thanks for your time.

[1:00:41] **Aviral:** Likewise, it was an absolute pleasure talking to all of you.

[1:00:45] **Blaze:** Thank you. Bye.

[1:00:46] **Rhea:** Thank you for your time. Bye, take care.

[1:00:47] **Aviral:** Um, does anyone have time to get me, get give me a quick idea of the next steps? Um,

[1:00:53] **Daniel:** Yeah, yeah, sure, sure. Yeah. Um, if you need to go, Rhea, feel free to, but yeah.

[1:00:58] **Rhea:** I'm good. I have time.

[1:00:59] **Daniel:** Yeah. Cool. Uh, so from this, uh, you'll probably get back. Um, you'll get contacted by the recruiter. I'm not sure if it was Rachel or...

[1:11] **Aviral:** I think, uh, Tyler was filling in for someone who had, yeah.

[1:13] **Daniel:** Tyler, yeah, he'll probably get back to you, uh, probably scheduling a second panel interview. Like we're interviewing multiple candidates, of course, but I think that you fit very well on what we're looking for. So probably you'll get scheduled for that second panel, which will be kind of similar to this one, but more technical. So it will be more about coding skills essentially. Like, uh, half oriented to AI machine learning and half just general programming, like, uh, if you're familiar, like C++, C#, how how strong are you on those?

[1:59] **Aviral:** Um, it would be like Python is my go-to language because that's something I've used the most. Um, then, um, on front end side, it would be like more TypeScript, JavaScript, but, um, like I've dealt with like so many languages at, um, like Amazon. But it would be more so like, um, like I I haven't like, I wouldn't call myself expert, but I've, I actually used seven different languages at Amazon. So we would pick it up on the way, but if you were to quickly ask me and do some whiteboarding, it would be Python.

[1:29] **Daniel:** Yeah, no, so so this would be more of a general programming questions, like how to, I don't know, optimize sorting, you know, typical questions about sorting a list or how can you implement, uh, I don't know, object-oriented thing adapting this to that or questions like more technical. And that would be all. That that's, that that would be like the next step and then after that, it would be a yes or no. Like, let's let's hope it it ends up being yes, but, uh, it depends on many factors.

[1:30:06] **Aviral:** Of course. Yeah. Okay. That that's good to hear. Um, it, uh, so so like if I understand correctly, it would be like the data structures, algorithm, sort list, inverted binary tree, that kind of thing.

[1:30:17] **Daniel:** Yeah, not not necessarily. Maybe like just technical questions. It could you could have programming questions like hands-on programming or maybe not, just questions about, I don't know, like how parameters are set up in a specific machine learning environment or I don't know really, like

[1:30:39] **Aviral:** Right. Yeah, I was going to ask quickly about like the machine learning itself because like I've had a chance to deal with machine learning like as someone else creating the models. Um, from my own side, like I have a little bit of experience on prototyping things and playing around with it, but I've left that to more data science roles. But I'm, I'm wondering that's why it's more, is it more so like, um, yeah, like like what the machine learning part would be? Cuz I'm used to like data structures questions or object-oriented programming, um, this one would be like, I'm curious about so that I can set myself in the proper...

[1:31:16] **Daniel:** Yeah, I mean, I don't have a clear answer for you because I honestly don't know what they would ask. Right. But, uh, for this role specifically, I think that you have what is needed. Like I I don't see very technical questions like deep C++, memory management. I don't think that they get into that. Right. I mean, I I would be talking with those guys, so I I I'll make sure that it's not the case. And things like maybe general app development, like how would you design the architecture of an app? Things like this, you know, right. Like, um, model view controller, maybe, you know...

[1:31:58] **Aviral:** The design patterns.

[1:31:59] **Daniel:** Yeah, general, exactly, design patterns, things like that. Yeah.

[1:32:03] **Aviral:** Okay. I appreciate that. Uh, yeah, that that is very helpful. It just like will put me at least in the right mindset and even if there's something that I don't immediately know, I can definitely walk through how I would go about solving that and the thought process of it.

[1:32:17] **Daniel:** Exactly. Exactly. That that's the most important. It's not about if you know the syntax, it's more about your general way of thinking, like designing a solution, exactly.

[1:32:27] **Aviral:** Hundred percent. Well, I appreciate that answer, Daniel.

[1:32:30] **Daniel:** Yeah, no problem.

[1:32:30] **Aviral:** Is there anything else I should know, uh, before we part ways?

[1:32:34] **Daniel:** No, I think I think it's all good. At this point, we we appreciate your time and and your clear and friendly way of explaining things. So, yeah, we hope we see you again in not a long time.

[1:32:51] **Aviral:** Absolutely. It was an absolute pleasure. Rhea, please go ahead. I always...

[1:32:57] **Rhea:** No no no, it's cool.

[1:32:58] **Aviral:** Oh no, I was just saying, it was, it was a lot of fun explaining all these details and learning about EA. Thank you so much.

[1:33:06] **Rhea:** Cool. Thank you.

[1:33:07] **Daniel:** Thank you for your time.

[1:33:09] **Rhea:** Thank you. Bye.

[1:33:09] **Aviral:** Take care. Bye.