# -*- coding: utf-8 -*-
"""Aviral_Garg_volcanoML - Aviral Garg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vRJ8frH6nK8RIo4PPJdRRmmw_Pz93u0M

# VolcanoML - Forecasting Volcanic Eruption and Impact Assessment

# Submission Meta Data

| Category                                        | Details                                                         |
|-------------------------------------------------|-----------------------------------------------------------------|
| **Full Name**                                   | Aviral Garg                                                     |
| **Uplevel Email Address**                       | gaviral.dev@gmail.com                                           |
| **Name of the Problem Statement of Submission** | VolcanoML (Forecasting Volcanic Eruption and Impact Assessment) |

# Python & Pip Version Check

> **Important Note:**
>
> This Jupyter Notebook works successfully works with `Python 3.10.12` and `pip 24.0`
"""

!python -V
!pip -V

!pip install --upgrade pip
!python -V
!pip -V

"""## System Information"""

import platform

import psutil

# Get the number of physical cores
physical_cores = psutil.cpu_count(logical=False)

# Get the number of logical cores (including hyper-threading)
logical_cores = psutil.cpu_count(logical=True)

# Check if hyper-threading is available
hyper_threading = logical_cores > physical_cores

# Print the results
print(f"Number of physical cores: {physical_cores}")
print(f"Number of logical cores: {logical_cores}")
print(f"Hyper-threading available: {hyper_threading}")

# Get additional CPU information
cpu_info = platform.processor()
print(f"\nCPU Information: {cpu_info}")

N_JOBS = logical_cores

"""# Requirements Engineering

## Requirements Engineering: **Criteria 1** (**C1**): Problem Understanding

*As per IK's grading rubric, the following aspects will be evaluated for Problem Understanding criteria*:

- Evaluate the understanding of the problem statement, tasks, and the importance of data transformation and structuring.

*Tabular Representation + checklist for self-evaluation*:

| Requirement Code | Requirement                                                              | Status |
|:----------------:|:-------------------------------------------------------------------------|:------:|
|    **C1.R1**     | Demonstrate *understanding* of all aspects of the **problem statement**. |   ✅    |
|    **C1.R2**     | Demonstrate *understanding* of the **tasks** to be performed.            |   ✅    |
|    **C1.R3**     | Demonstrate *importance* of **data transformation and structuring**.     |   ✅    |



## Requirements Engineering: **Criteria 2** (**C2**): Data Preparation and Feature Engineering

*As per IK's grading rubric, the following aspects will be evaluated for Data Preparation and Feature Engineering criteria*:

- Check if the candidate applies appropriate techniques for data preprocessing, including normalization and outlier removal.
- Evaluate the effectiveness of feature engineering techniques applied.
- Assess the proficiency in utilizing feature selection and extraction, to enhance the predictive power of the data.
- Check for novel approaches, techniques, or insights that go beyond standard practices.
- Assess if the student has considered the temporal nature of the data and implemented any relevant techniques for time series analysis.

*Tabular Representation + checklist for self-evaluation*:

| Requirement Code | Requirement                                                                                                                      | Status |
|:----------------:|:---------------------------------------------------------------------------------------------------------------------------------|:------:|
|    **C2.R1**     | Demonstrate *application* of appropriate **data preprocessing techniques**, including **normalization** and **outlier removal**. |   ✅    |
|    **C2.R2**     | Demonstrate *effectiveness* of the applied **feature engineering techniques**.                                                   |   ✅    |
|    **C2.R3**     | Demonstrate *proficiency* in utilizing **feature selection** and **extraction** to *enhance predictive power*.                   |   ✅    |
|    **C2.R4**     | Demonstrate *application* of *novel* **approaches**, **techniques**, or **insights** that *go beyond standard practices*.        |   ✅    |
|    **C2.R5**     | Demonstrate *consideration* of the **temporal nature of the data**.                                                              |   ✅    |
|    **C2.R6**     | Demonstrate *implementation* of relevant **techniques for time series analysis**.                                                |   ✅    |

## Requirements Engineering: **Criteria 3** (**C3**): Regression Technique Selection and Model Training

*As per IK's grading rubric, the following aspects will be evaluated for Regression Technique Selection and Model Training criteria*:

- Evaluate the performance of the regression models in estimating the time left for eruption and in estimating the magnitude of the impending volcanic eruption.
- Assess the ability to conduct proper model training, including hyperparameter tuning and optimization, using suitable techniques like cross-validation.
- Check if the candidate has taken advantage of features such as automated model selection, hyperparameter tuning, and performance evaluation.

*Tabular Representation + checklist for self-evaluation*:

| Requirement Code | Requirement                                                                                                                                                                | Status |
|:----------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------:|
|    **C3.R1**     | Demonstrate *performance* of the **regression models** in ***estimating the time left for eruption***.                                                                     |   ✅    |
|    **C3.R2**     | Demonstrate *performance* of the **regression models** in ***estimating the magnitude of the impending volcanic eruption***.                                               |   ✅    |
|    **C3.R3**     | Demonstrate *ability to conduct* proper **model training**, including **hyperparameter tuning** and **optimization**, using suitable techniques like **cross-validation**. |   ✅    |
|    **C3.R4**     | Demonstrate *utilization* of features such as **automated model selection**, **hyperparameter tuning**, and **performance evaluation**.                                    |   ✅    |

## Requirements Engineering: **Criteria 4** (**C4**): Model Evaluation and Performance Metrics

*As per IK's grading rubric, the following aspects will be evaluated for Model Evaluation and Performance Metrics criteria*:

- Consider the suitability of the regression models in capturing the relationship between the tilt erupt value and the eruption magnitude.
- Evaluate the usage of regression models using relevant metrics, such as mean squared error (MSE), root mean squared error (RMSE), or R-squared value.

*Tabular Representation + checklist for self-evaluation*:

| Requirement Code | Requirement                                                                                                                                                                | Status |
|:----------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------:|
|    **C4.R1**     | Demonstrate *suitability* of the **regression models** in capturing the relationship between the **tilt erupt** value and the **eruption magnitude**.                      |   ✅    |
|    **C4.R2**     | Demonstrate *usage* of **regression models** using relevant **metrics**, such as **mean squared error (MSE)**, **root mean squared error (RMSE)**, or **R-squared value**. |   ✅    |

## Requirements Engineering: **Criteria 5** (**C5**): Result Analysis and Model Refinement

*As per IK's grading rubric, the following aspects will be evaluated for Result Analysis and Model Refinement criteria*:

- Implements appropriate refinements based on the analysis, such as adjusting hyperparameters, feature selection, or ensemble techniques.
- Consider the uasge of techniques such as feature importance scores or model interpretability methods that capture the relevant information from the features.

*Tabular Representation + checklist for self-evaluation*:

| Requirement Code | Requirement                                                                                                                                                                    | Status |
|:----------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------:|
|    **C5.R1**     | Demonstrate *implementation* of *appropriate refinements* based on the **analysis**, such as **adjusting hyperparameters**, **feature selection**, or **ensemble techniques**. |   ✅    |
|    **C5.R2**     | Demonstrate *usage* of **techniques** such as **feature importance scores** or **model interpretability methods** that *capture the relevant information from the features*.   |   ✅    |

## Requirements Engineering: **Criteria 6** (**C6**): Validation and Generalization

*As per IK's grading rubric, the following aspects will be evaluated for Validation and Generalization criteria*:

- Assess if the student has demonstrated innovation and creativity in addressing the regression tasks.
- Evaluate if the candidate has proposed techniques or strategies to ensure the models' generalizability and adaptability to diverse volcanic environments.

*Tabular Representation + checklist for self-evaluation*:

| Requirement Code | Requirement                                                                                                                                                      | Status |
|:----------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------|:------:|
|    **C6.R1**     | Demonstrate *innovation* and *creativity* in addressing the **regression tasks**.                                                                                |   ✅    |
|    **C6.R2**     | Demonstrate *proposal* of **techniques** or **strategies** to ensure the *models*' **generalizability** and **adaptability** to *diverse volcanic environments*. |   ✅    |

## Requirements Engineering: **Criteria 7** (**C7**): Documentation and Reporting

*As per IK's grading rubric, the following aspects will be evaluated for Documentation and Reporting criteria*:

- Evaluate if the candidate effectively communicates the methodology, results, and insights in a clear and organized manner.

*Tabular Representation + checklist for self-evaluation*:

| Requirement Code | Requirement                                                                                  | Status |
|:----------------:|:---------------------------------------------------------------------------------------------|:------:|
|    **C7.R1**     | Demonstrate *effective communication* of the **methodology**, **results**, and **insights**. |   ✅    |

---

# Problem Understanding

<!--
## Motivation

Given instructions: Volcanic eruptions pose significant risks to both human life and infrastructure. Early and accurate prediction of volcanic eruptions can enable timely evacuation and mitigation measures, potentially saving lives and minimizing damage. Machine learning techniques have shown promise in various domains, including regression tasks, and can potentially aid in forecasting volcanic eruptions.
-->

## Problem Understanding > Motivation

- *Early* and *accurate* prediction of volcanic eruptions can mitigate risks to both human life and infrastructure.
- It is a great use case for ML.

<!--
## Introduction
-->

<!--
Obtaining sensor readings from volcanic eruptions poses a significant challenge due to the limited number of active volcanoes worldwide and the infrequency of their eruptions. However, a collaboration between researchers at NVIDIA and NTU Singapore has yielded a promising solution. They have developed a mathematical model capable of simulating volcanic eruptions, enabling the generation of synthetic sensor readings. This innovative approach addresses the scarcity of real-world data by providing a reliable means of studying volcanic activity and its associated phenomena.

The [synthetically generated dataset](https://drive.google.com/file/d/1lNJLR0r8zxE-EU1H4f57OWLXr9agxBwb/view?usp=sharing) is a collection of diverse volcanoes. By altering the parameters of the mathematical model, synthetic observations are generated for each volcano. These parameters include Poisson's ratio (v), atmospheric pressure (Patm), gravity (g), radial distance tilt station (r), Sheer modulus (G), Magma density (rho), Magma viscosity (mu), conduit radius (rc), gas mass (M), standard deviation (sigma), tilt angle at eruption (tilt_erupt), and sensor values preceding the eruption event. An example observation is shown below.
-->


<!--
***
```txt
v,0.25
Patm,100000
g,9.81
r,500
G,10^11.12
rho,2200
mu,10^3.51
rc,18

M,10^6.18
sigma,0.1

tilt_erupt 3.1271 nrad

-1052, -1051, -1050, ... , -2,    -1,    0
v1,    v2,    v3,    ... , v1051, v1052, v1053
```
***

The sensor reading labelled as v1 denotes the measurement captured 1053 seconds before the eruption, corresponding to time step -1052. The last value, v1053, represents the tilt erupt value, indicating the sensor reading precisely at the moment of eruption, after which the sensor is inoperable/destroyed. In a broader context, when an observation contains N readings, the sensor reading v(i) represents the measurement taken (N - i) seconds before the eruption. In the actual dataset, the last two lines will not have any spaces between the values and the commas. The spaces are added here for better readability.

The goal is to evaluate regression techniques' suitability for anticipating and comprehending volcanic phenomena using the synthetically generated dataset. The objective is to determine the effectiveness of these techniques in improving the understanding and predictive capabilities related to volcanic activities.


**File**: `Volcano1/observation1.txt`

```txt

v,0.25
Patm,100000
g,9.81
r,500
G,10^11.12
rho,2200
mu,10^3.51
rc,18

M,10^6.18
sigma,0.1

tilt_erupt,3.1271nrad

-1052,-1051,-1050,...,-2,-1,0
-4.87775135323027e-12,-5.41769735416295e-11,6.28908620278579e-11,...,2.39097015527473e-09,2.74650112407467e-09,3.21582418048671e-09
```
-->

## Problem Understanding > Input Data Analysis

- ML requires an abundance of data.
- Instead, there is a scarcity of real-world because volcanic eruptions are rare.
- Therefore, we'll use [**synthetic sensor readings dataset**](https://drive.google.com/file/d/1lNJLR0r8zxE-EU1H4f57OWLXr9agxBwb/view?usp=sharing) generated by a reliable mathematical model that *simulates* volcanic eruptions.

Following input data is available:

- 10 `Volcano*` directories representing 10 different volcanoes.
    - 3-30 `observation*.txt` files in each `Volcano*` directory.
        - Each observation contains **Volcano Parameters** and **Time Series** of sensor readings leading to the eruption event.
            - **Volcano Parameters**
                - Poisson's ratio (v)
                - atmospheric pressure (Patm)
                - gravity (g)
                - radial distance tilt station (r)
                - Sheer modulus (G)
                - Magma density (rho)
                - Magma viscosity (mu)
                - conduit radius (rc)
                - gas mass (M)
                - standard deviation (sigma)
                - tilt angle at eruption (tilt_erupt) (*to be ignored*)
            - **Time Series**
                - time-series of the tilt angle sensor readings leading to the eruption event.
                    - time steps (in seconds)
                    - sensor readings (in radians)

- **Example observation** (File: `Volcano1/observation1.txt`):

| Line Number | Content                                          | Description                                                                                                                                       |
|:-----------:|:-------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|
|   Line 1    | `v,0.25`                                         | Poisson's ratio (v)                                                                                                                               |
|   Line 2    | `Patm,100000`                                    | Atmospheric pressure (Patm)                                                                                                                       |
|   Line 3    | `g,9.81`                                         | Gravity (g)                                                                                                                                       |
|   Line 4    | `r,500`                                          | Radial distance tilt station (r)                                                                                                                  |
|   Line 5    | `G,10^11.12`                                     | Sheer modulus (G)                                                                                                                                 |
|   Line 6    | `rho,2200`                                       | Magma density (rho)                                                                                                                               |
|   Line 7    | `mu,10^3.51`                                     | Magma viscosity (mu)                                                                                                                              |
|   Line 8    | `rc,18`                                          | Conduit radius (rc)                                                                                                                               |
|   Line 9    |                                                  | ---- ***Empty Line*** ----                                                                                                                        |
|   Line 10   | `M,10^6.18`                                      | Gas mass (M)                                                                                                                                      |
|   Line 11   | `sigma,0.1`                                      | Standard deviation (sigma)                                                                                                                        |
|   Line 12   |                                                  | ---- ***Empty Line*** ----                                                                                                                        |
|   Line 13   | `tilt_erupt,3.1271nrad`                          | Tilt angle at eruption (tilt_erupt): <br>Instructions are to ignore this and, instead, use the last sensor reading (last value in the last line). |
|   Line 14   |                                                  | ---- ***Empty Line*** ----                                                                                                                        |
|   Line 15   | `-1052, -1051, -1050, ... , -2,    -1,    0`     | Comma-separated time steps (in seconds) leading to the eruption event (0). <br>Number of time-steps varies for each observation.                  |
|   Line 16   | `v1,    v2,    v3,    ... , v1051, v1052, v1053` | Comma-separated sensor readings at each time step. The last value represents the tilt erupt value.                                                |
|   Line 17   |                                                  | ---- ***Empty Line*** ----                                                                                                                        |

Here, `...` has been used to indicate that the sensor values continue for a long time. The second-last line contains the number of seconds before the eruption event. The last line contains the sensor values. It is a **time series** of sensor readings.

Table below shows the last 10 sensor readings for the example observation:

| Time Steps                    | -10                  | -9                  | -9                   | -7                   | -6                  | -5                   | -4                   | -3                   | -2                   | -1                   | 0                    |
|-------------------------------|----------------------|---------------------|----------------------|----------------------|---------------------|----------------------|----------------------|----------------------|----------------------|----------------------|----------------------|
| Sensor Reading representation | v1043                | v1044               | v1045                | v1046                | v1047               | v1048                | v1049                | v1050                | v1051                | v1052                | v1053                |
| Sensor Readings               | 5.87645678652353e-10 | 5.7137404004562e-10 | 1.10218959674131e-09 | 9.03221277194527e-10 | 1.1565339347471e-09 | 1.60139768717403e-09 | 1.60982352250034e-09 | 1.98543099256586e-09 | 2.39097015527473e-09 | 2.74650112407467e-09 | 3.21582418048671e-09 |

<!--
Within Table 1, the sensor reading labelled as v1 denotes the measurement captured 11 seconds before the eruption, corresponding to time step -10. The last value, v11, represents the tilt erupt value, indicating the sensor reading precisely at the moment of eruption. In a broader context, when an observation contains N readings, the sensor reading v(i) represents the measurement taken (N - i) seconds before the eruption.
-->

The sensor reading labelled as `v1043` (actual value: `5.87645678652353e-10`) denotes the measurement captured 11 seconds before the eruption, corresponding to time step `-10`. The last value, `v1053` (actual value: `3.21582418048671e-09`), represents the tilt erupt value, indicating the sensor reading precisely at the moment of eruption. In a broader context, when an observation contains N readings, the sensor reading `v(i)` represents the measurement taken `(N - i)` seconds before the eruption.

<!--
The goal is to evaluate regression techniques' suitability for anticipating and comprehending volcanic phenomena using the synthetically generated dataset. The objective is to determine the effectiveness of these techniques in improving the understanding and predictive capabilities related to volcanic activities.

The goal is:
    - to evaluate regression techniques' suitability for anticipating and comprehending volcanic phenomena using the synthetically generated dataset.
The objective is:
    - to determine the effectiveness of these techniques in improving the understanding and predictive capabilities related to volcanic activities.
-->

<!--
## Problem Statement

The problem at hand entails analyzing a sequence of sensor readings taken at consecutive time steps to address two distinct tasks related to volcanic eruptions. Firstly, the aim is to devise an effective approach that accurately estimates the remaining time units until eruption for a given observation. This observation can have sensor readings of varying lengths. By leveraging the information within the sensor data, the objective is to provide timely predictions for the impending eruption, enabling proactive measures to be taken to ensure the safety of affected areas and populations.

The second task involves estimating the magnitude of the impending volcanic eruption. Utilizing the provided observation, the goal is to develop a methodology that can estimate the severity and intensity of the eruption event. Despite the absence of the eruption magnitude in the observations, it is possible to estimate the tilt erupt value, which represents the last sensor reading corresponding to the moment of eruption. This particular value exhibits a strong correlation with the magnitude of the eruption. By estimating the tilt erupt value, we can obtain a reliable estimation of the eruption's impact. The obtained estimation is crucial for emergency management and response planning, allowing authorities to allocate appropriate resources and implement necessary measures to mitigate the potential impacts of the volcanic eruption on surrounding areas.


## Problem Statement

The problem entails **analyzing a sequence of sensor readings taken at consecutive time steps** to address **two distinct tasks** related to volcanic eruptions.

- **Task 1**: Estimate the **remaining time units** until eruption for a given observation.
    - **Notes**:
        - The observation can have sensor readings of varying lengths.
        - Extract information from the sensor data to provide timely predictions for the impending eruption.
- **Task 2**: Estimate the magnitude of the impending volcanic eruption.
    -  **Notes**:
        - The observation does not contain the eruption magnitude.
        - Estimate the `tilt_erupt` value, which represents the last sensor reading corresponding to the moment of eruption.
        - The tilt erupt value exhibits a strong correlation with the magnitude of the eruption.
        - By estimating the `tilt_erupt` value, we can obtain a reliable estimation of the eruption's impact.
-->

<!--
## Procedure

- ***Data Preparation***: Preprocess the raw dataset to generate labeled samples. Split the generated samples into training and testing subsets. Consider any necessary feature engineering techniques to enhance the predictive power of the data.
- ***Regression Technique Selection***: Identify and select suitable regression techniques to be evaluated for their effectiveness in anticipating and comprehending volcanic phenomena. This may include popular methods such as linear regression, decision trees, random forests, support vector regression, boosting algorithms, or neural networks.
- ***Model Training***: Train the selected regression models on the training subset of the dataset. Adjust hyperparameters and perform model optimization techniques, such as cross-validation, to ensure optimal performance. During the process of cross-validation, it is advisable to ensure an unbiased estimate of performance measures by considering different volcanoes in both the train and test sets. This approach helps to mitigate any potential biases that may arise from overfitting to specific volcano data, allowing for a more robust evaluation of the regression model's performance.
- ***Model Evaluation***: Evaluate the trained regression models using the testing subset of the dataset. Measure their performance using appropriate evaluation metrics such as mean squared error, root mean squared error, or R-squared value.
- ***Results Analysis***: Analyze the performance of the regression models and compare their effectiveness in anticipating and comprehending volcanic phenomena. Identify the most suitable techniques that yield the best predictive capabilities.
- ***Model Refinement***: Fine-tune the selected regression model(s) based on the analysis of the results. This may involve adjusting hyperparameters, incorporating feature selection methods, or exploring ensemble techniques to further enhance performance.
- ***Final Evaluation***: Validate the refined regression model(s) on unseen data or real-world observations, if available. Assess the model's ability to anticipate and comprehend volcanic phenomena accurately.
- ***Discussion and Conclusion***: Summarize the findings, limitations, and implications of the study. Discuss the potential applications of the regression model(s) in enhancing the understanding and prediction of volcanic activities.
- ***Further Research***: Identify any areas for improvement or additional research that could enhance the accuracy and effectiveness of regression techniques in anticipating and comprehending volcanic phenomena.

- **Procedure** (provided by the instructor):
    - **Data Preparation**:
        - _Preprocess_ the raw dataset to generate **labeled samples**.
        - _Split_ the generated samples into **training** and **testing subsets**.
        - _Consider_ any necessary **feature engineering** techniques to enhance the predictive power of the data.
    - **Regression Technique Selection**:
        - _Identify_ and _select_ suitable **regression techniques** to be evaluated for their effectiveness in anticipating and comprehending volcanic phenomena.
        - Regression techniques may include popular methods such as:
            - linear regression
            - decision trees
            - random forests
            - support vector regression
            - boosting algorithms
            - neural networks
    - **Model Training**:
        - _Train_ the selected regression models on the **training subset** of the dataset.
        - _Adjust_ **hyperparameters** and perform **model optimization** techniques, such as **cross-validation**, to ensure optimal performance.
        - **Notes**:
            - During the process of cross-validation, it is advisable to ensure an unbiased estimate of performance measures by considering different volcanoes in both the train and test sets.
            - This approach helps to mitigate any potential biases that may arise from overfitting to specific volcano data, allowing for a more robust evaluation of the regression model's performance.
    - **Model Evaluation**:
        - _Evaluate_ the trained regression models using the **testing subset** of the dataset.
        - _Measure_ their performance using appropriate **evaluation metrics** such as:
            - mean squared error
            - root mean squared error
            - R-squared value
    - **Results Analysis**:
        - _Analyze_ the **performance of the regression models** and _compare_ their **effectiveness** in anticipating and comprehending volcanic phenomena.
        - _Identify_ the **most suitable techniques** that yield the best predictive capabilities.
    - **Model Refinement**:
        - _Fine-tune_ the selected regression model(s) based on the analysis of the results.
        - _This may involve_:
            - adjusting hyperparameters
            - incorporating feature selection methods
            - exploring ensemble techniques to further enhance performance
    - **Final Evaluation**:
        - _Validate_ the refined regression model(s) on **unseen data** or **real-world observations**, if available.
        - _Assess_ the model's ability to anticipate and comprehend volcanic phenomena accurately.
    - **Discussion and Conclusion**:
        - _Summarize_ the **findings**, **limitations**, and **implications** of the study.
        - _Discuss_ the potential applications of the regression model(s) in enhancing the understanding and prediction of volcanic activities.
    - **Further Research**:
        - _Identify_ any **areas for improvement** or **additional research** that could enhance the accuracy and effectiveness of regression techniques in anticipating and comprehending volcanic phenomena.
- **Additional Pointers** (provided by the instructor):
    - **Transform: Unstructured** raw data into a **structured** representation.
        - **Notes**:
            - There are various approaches to accomplish this task.
            - One option is to construct forecasting models using a fixed-length observation.
            - Alternatively, you can consider feature engineering techniques to build a dataset from either variable or fixed-length observations.
            - These strategies provide flexibility in handling the data and allow for effective modeling and analysis.
        - _Consider_ the package `tsfresh` for feature engineering for time-series data.
    - **Data Availability**:
        - _Augment_ the dataset synthetically.
        - _Consider_ the following approach:
            - Extract multiple variable-length subsequences from a single observation, thereby augmenting the data.
            - By adopting this technique, you can enhance the dataset's size and diversity, which can be beneficial for training robust models.
    - **Evaluation**:
        - _Take into account_ the varying lengths of observations when splitting the data into training and testing sets to ensure a representative evaluation.
    - **Estimation Process**:
        - _Normalize_ engineered features.
        - _Remove_ outliers.
        - _Retain_ only pertinent features in the dataset.
        - _Notes_:
            - By eliminating outliers, the impact of extreme or erroneous values can be mitigated, resulting in more accurate and reliable estimates.
            - Selecting relevant features ensures that the estimation model focuses on the most informative variables, thereby improving the quality of the estimates.
    - **Tilt-erupt Prediction Task**:
        - _Consider_ the box-cox transformation for your output as the magnitude of sensor readings is very small.
    - **Future Research**:
        - The provided time-series observations exhibit temporal patterns, which are not fully utilized by traditional supervised regression models.
        - To capture and effectively model these patterns, it is worthwhile to explore alternative approaches.
        - These include traditional time-series forecasting methods, deep learning-based approaches, state space methods, and popular Python libraries like Prophet.
        - By considering a diverse range of techniques, it becomes possible to achieve reliable and accurate forecasting results that cater to the specific characteristics and requirements of the time series data.
        - It is important to note that each method possesses its own strengths and assumptions, and the selection of an appropriate method should be guided by the unique characteristics of the time series data and the specific forecasting objectives of the project.
    - **Note**: Use the last sensor reading of an observation as the tilt-erupt value. Ignore the tilt-erupt value specified in the volcano parameters.
-->

The goal here is to:
- Evaluate regression techniques' suitability for anticipating and comprehending volcanic phenomena using the synthetically generated dataset.
- Determine the effectiveness of these techniques in improving the understanding and predictive capabilities related to volcanic activities.

## Problem Understanding > Problem Statement and Task Analysis

The problem entails **analyzing a sequence of sensor readings taken at consecutive time steps** to address **two distinct tasks** related to volcanic eruptions:

- **Task 1**: Estimate the **remaining time units** until eruption for a given observation.
    - **Notes**:
        - The observation can have sensor readings of varying lengths.
        - Extract information from the sensor data to provide timely predictions for the impending eruption.
- **Task 2**: Estimate the magnitude of the impending volcanic eruption.
    - **Notes**:
        - The observation does not contain the eruption magnitude.
        - Estimate the `tilt_erupt` value, which represents the last sensor reading corresponding to the moment of eruption.
        - The tilt erupt value exhibits a strong correlation with the magnitude of the eruption.
        - By estimating the `tilt_erupt` value, we can obtain a reliable estimation of the eruption's impact.

## Problem Understanding > Initial Dynamic Experiment Plan (evolves with data characteristics exploration)

To perform these tasks, the experiment plan is as follows:


- **Problem Understanding**:
    - Assess the understanding of the problem statement, tasks, and the importance of data transformation and structuring. (C1.R1, C1.R2, C1.R3)
- **Data Preparation**:
    - Preprocess the dataset to generate labeled samples and split these into training and testing subsets.
    - Apply necessary feature engineering techniques to enhance the predictive power of the data, including normalization of features, outlier removal, and feature selection.
    - **Data Identification**: Leverage the synthetically generated dataset, emphasizing the importance of overcoming real-world data scarcity.
    - **Data Cleaning**:
        - Implement steps for handling missing values and treating outliers to ensure high data quality.
        - **Note**: Use the last sensor reading of an observation as the tilt-erupt value. Ignore the tilt-erupt value specified in the volcano parameters.
        - Remove constants and near-constants from the dataset.
        - _Remove_ outliers: This step involves identifying and excluding anomalies or extreme values that might skew the analysis and model training, ensuring the robustness and generalizability of the machine learning models. It's crucial for improving model accuracy and performance by focusing on more representative data.
            - By eliminating outliers, the impact of extreme or erroneous values can be mitigated, resulting in more accurate and reliable estimates.
    - **Data Transformation: Unstructured to Structured**: Convert raw sensor data into a structured format suitable for analysis, using techniques and tools appropriate for time-series data, like tsfresh.
    - **Data Augmentation**:
        - Generate variable-length observation sequences through synthetic data generation, enhancing dataset robustness and diversity.
        - Augment the dataset synthetically by extracting multiple variable-length subsequences from single observations to increase data size and diversity.
    - Given the characteristics of the data, _Preprocess_ the raw dataset to generate **labeled samples**.
        - **Data Transformation: Unstructured** raw data into a **structured** representation.
            - We can consider multiple techniques to have flexibility in handling the data and to pave the road for effectiveness in the later steps of modeling and analysis.
            - For that, let's compare the impact of the following strategies:
                - *Experiment Option 1*: **fixed-length observation** forecasting models.
                - *Experiment Option 2*: build dataset from **variable-length observations**.
                - *Experiment Option 3*: combination of both options, 1 & 2.
    - **Data Preprocessing**: _Preprocess_ the raw dataset to generate **labeled samples**. (Criteria 2 - C2.R1: Data preprocessing techniques)
    - **Data Validation**: _Validate_ the input data to ensure it follows the specified format.
    - **Data Transformation**: Given the unstructured nature of the raw data, **transform** it into a **structured representation**, considering different strategies for handling fixed and variable-length observations. (C2.R5, C2.R6: Temporal nature and time series analysis)
        - **Approaches**: There are various approaches to accomplish this task:
            - Construct forecasting models using a fixed-length observation.
            - Consider feature engineering techniques to build a dataset from either variable or fixed-length observations.
        - **Feature Engineering for Time-Series**: _Consider_ using packages like **tsfresh** for feature engineering. (C2.R2: Effectiveness of feature engineering)
    - **Synthetic Data Augmentation**: _Augment_ the dataset by extracting multiple variable-length subsequences from a single observation to enhance the size and diversity of the training set. (C2.R4: Novel approaches)
    - **Data Splitting**: _Split_ the generated samples into **training** and **testing subsets**, ensuring to accommodate the varying lengths of observations for a representative evaluation. (C4.R1: Model suitability)
        - Consider splitting the data into 3 train-test-validate and compare.
- **Feature Engineering**:
    - **Output Transformation**: Applying the box-cox transformation to the output variable (in this case, the tilt-erupt value) falls under output transformation, aimed at stabilizing variance and making the data more closely meet the assumptions of many machine learning algorithms, particularly those that assume normally distributed errors. By transforming the output variable to be more normally distributed, models may predict more accurately, especially for regression tasks where the scale and distribution of the target variable can significantly impact performance.
    - **Feature Engineering Insight**: _Consider_ any necessary **feature engineering** techniques to enhance the predictive power of the data, with the flexibility of handling both variable and fixed-length observations.
    - _Normalize_ engineered features: Normalization is the process of scaling numerical features to a uniform range, such as 0 to 1 or -1 to 1, making training more stable and convergence faster. This step is particularly important for models sensitive to the scale of input features, such as neural networks and models that use gradient descent for optimization.
    - _Consider_ any necessary **feature engineering** techniques to enhance the predictive power of the data.
    - Explore the impact of using **tsfresh** given the time-series characteristic of the input data.
    - **Feature Selection**: This process involves identifying and retaining the most informative and relevant features for use in model training, while discarding those that are unlikely to contribute to the model's predictive power. Retaining only pertinent features helps in reducing model complexity, improving interpretability, enhancing generalization by reducing overfitting, and often leading to better model performance. It's a critical step for focusing the model's attention on the data aspects that are most significant for the task at hand.
        - Selecting relevant features ensures that the estimation model focuses on the most informative variables, thereby improving the quality of the estimates.
    - **Output Transformation**: Applying the box-cox transformation to the output variable (in this case, the tilt-erupt value) falls under output transformation, aimed at stabilizing variance and making the data more closely meet the assumptions of many machine learning algorithms, particularly those that assume normally distributed errors. By transforming the output variable to be more normally distributed, models may predict more accurately, especially for regression tasks where the scale and distribution of the target variable can significantly impact performance.
- **Regression Technique Selection**:
    - **Technique Identification and Selection**: _Identify_ and _select_ suitable **regression techniques** to be evaluated for their effectiveness in anticipating and comprehending volcanic phenomena, including the **box-cox transformation** for tilt-erupt prediction due to small sensor reading magnitudes. (Criteria 3 - C3: Technique selection and model training)
    - Regression techniques may include popular methods such as:
        - **Linear Regression**: linear regression
        - **Decision Trees**: decision trees
        - **Random Forests**: random forests
        - **Support Vector Regression**: support vector regression
        - **Boosting Algorithms**: boosting algorithms
        - **Neural Networks**: neural networks
    - **Method Overview**: Regression techniques may include popular methods such as (C3.R1, C3.R2):
- **Model Training**:
    - **Model Training Process**: _Train_ the selected regression models on the **training subset** of the dataset. (C3.R3, C3.R4: Model training and hyperparameter tuning)
    - **Hyperparameter Tuning and Optimization**: _Adjust_ **hyperparameters** and perform **model optimization** techniques, such as **cross-validation**, to ensure optimal performance. (C5.R1, C5.R2: Refinements and model interpretability)
        - **Cross-Validation Consideration**: During the process of cross-validation, it is advisable to ensure an unbiased estimate of performance measures by considering different volcanoes in both the train and test sets. This approach helps to mitigate any potential biases that may arise from overfitting to specific volcano data, allowing for a more robust evaluation of the regression model's performance.
    - **Feature Normalization and Outlier Removal**: _Normalize_ engineered features and _remove_ outliers to enhance accuracy and retain only **pertinent features** in the dataset. (Criteria 5 - C5.R1: Model refinement)
- **Model Evaluation**:
    - **Evaluation Process**: _Evaluate_ the trained regression models using the **testing subset** of the dataset. (Criteria 4 - C4: Model evaluation and performance metrics)
    - _Take into account_ the varying lengths of observations when splitting the data into training and testing sets to ensure a representative evaluation.
    - _Evaluate_ the trained regression models using the **testing subset** of the dataset.
    - **Performance Metrics**: _Measure_ their performance using appropriate **evaluation metrics** such as:
        - **Mean Squared Error**: mean squared error
        - **Root Mean Squared Error**: root mean squared error
        - **R-squared Value**: R-squared value
    - **Consideration of Observation Lengths**: Take into account the varying lengths of observations during evaluation for a representative assessment of the models.
- **Results Analysis**:
    - **Performance Analysis**: _Analyze_ the **performance of the regression models** and _compare_ their **effectiveness** in anticipating and comprehending volcanic phenomena.
    - **Technique Suitability**: _Identify_ the **most suitable techniques** that yield the best predictive capabilities. (Criteria 5 - C5: Result analysis and model refinement)
- **Model Refinement**:
    - **Refinement Steps**: _Fine-tune_ the selected regression model(s) based on the analysis of the results. (C5.R1: Appropriate refinements)
    - _This may involve_:
        - **Hyperparameter Adjustments**: adjusting hyperparameters
        - **Feature Selection**: incorporating feature selection methods
        - **Ensemble Method Exploration**: exploring ensemble techniques to further enhance performance
    - **Enhancement Techniques**: _Refine_ models by:
        - **Hyperparameter Adjustments**: adjusting hyperparameters
        - **Feature Selection**: incorporating feature selection methods
        - **Ensemble Method Exploration**: exploring ensemble techniques to further enhance performance
    - **Model Interpretability**: _Consider_ using **techniques** such as **feature importance scores** or **model interpretability methods** that _capture the relevant information from the features_. (C5.R2: Model interpretability methods)
- **Final Evaluation**:
    - **Validation Process**: _Validate_ the refined regression model(s) on **unseen data** or **real-world observations**, if available. (Criteria 6 - C6: Validation and generalization)
    - **Real-world Assessment**: _Assess_ the model's ability to anticipate and comprehend volcanic phenomena accurately using the last sensor reading as the tilt-erupt value. (C6.R2: Generalizability and adaptability)
    - **Note**: Use the last sensor reading of an observation as the tilt-erupt value. Ignore the tilt-erupt value specified in the volcano parameters.
- **Discussion and Conclusion**:
    - **Study Recapitulation**: _Summarize_ the **findings**, **limitations**, and **implications** of the study. (Criteria 7 - C7: Documentation and reporting)
    - **Application Potential**: _Discuss_ the potential applications of the regression model(s) in enhancing the understanding and prediction of volcanic activities.
- **Further Research**:
    - **Exploration of Temporal Patterns**: Address the temporal patterns in the time-series data by exploring alternative approaches for improved forecasting. (C2.R5, C2.R6: Time series analysis)
        - **Advanced Forecasting Methods**: Consider traditional time-series forecasting methods, deep learning-based approaches, state space methods, and libraries like Prophet.
    - **Improvement Areas and Future Directions**: _Identify_ any **areas for improvement** or **additional research** that could enhance the accuracy and effectiveness of regression techniques in anticipating and comprehending volcanic phenomena, going beyond traditional supervised regression models to utilize temporal patterns.
    - **Innovation and Creativity**: _Demonstrate_ **innovation** and **creativity** in addressing the **regression tasks**. (Criteria 6 - C6.R1: Innovation and creativity)
    - The provided time-series observations exhibit temporal patterns, which are not fully utilized by traditional supervised regression models.
        - To capture and effectively model these patterns, it is worthwhile to explore alternative approaches.
            - These include traditional time-series forecasting methods, deep learning-based approaches, state space methods, and popular Python libraries like Prophet.
        - By considering a diverse range of techniques, it becomes possible to achieve reliable and accurate forecasting results that cater to the specific characteristics and requirements of the time series data.
    - It is important to note that each method possesses its own strengths and assumptions, and the selection of an appropriate method should be guided by the unique characteristics of the time series data and the specific forecasting objectives of the project.
    - _Identify_ any **areas for improvement** or **additional research** that could enhance the accuracy and effectiveness of regression techniques in anticipating and comprehending volcanic phenomena.

---

<!-- ### 1: Data Acquisition and Understanding

- **Data Identification**: Identifying necessary data types, sources, and requirements.
- **Data Acquisition**: Gathering data from various sources (databases, APIs, files, web scraping).

### 2: Data Preparation

- **Data Integration**: Merging data from disparate sources, ensuring consistency.
  - **Data Cleaning**: Handling missing values, errors, and outliers.
  - **Data Validation**: Checking for data accuracy, relevance, and adherence to compliance standards.
- **Data Exploration and Analysis**: Preliminary analysis to understand data distributions, correlations, and potential quality issues.
  - **Data Visualization**: Using plots and charts to discover patterns or anomalies.
- **Data Annotation/Labeling**: For supervised learning, ensuring data is accurately labeled.

### 3: Data Preprocessing

- **Data Transformation**: Converting data into a suitable format for modeling.
  - **Normalization/Standardization**: Adjusting feature scales.
  - **Encoding**: Transforming categorical data into numeric formats.
- **Data Reduction**: Decreasing data complexity while preserving its essence.
  - **Feature Selection**: Choosing the most relevant features for the model.
  - **Dimensionality Reduction**: Applying techniques like PCA to reduce feature space.
- **Data Augmentation**: Enhancing the dataset by adding modified instances to increase diversity.
- **Feature Engineering**: Creating new features to improve model's ability to leverage information.
- **Data Balancing**: Addressing imbalances in the dataset to prevent model bias.

### 4: Model Preparation

- **Data Splitting**: Dividing the dataset into training, validation, and test sets.

### 5: Model Development and Evaluation

- **Model Selection**: Choosing appropriate algorithms based on the problem type and data characteristics.
- **Model Training**: Training models using the training set and selected features.
- **Model Evaluation**: Assessing model performance with validation and test sets.
  - **Cross-validation**: Using cross-validation techniques for more robust evaluation.
- **Hyperparameter Tuning**: Adjusting model parameters to optimize performance.
- **Model Validation**: Validating the model against new data to ensure generalizability.

### 6: Model Deployment and Monitoring

- **Deployment**: Deploying the model into a production environment.
- **Monitoring**: Continuously monitoring model performance and data drift.
- **Model Updating**: Refining and retraining the model as necessary based on performance metrics and feedback.

### 7: Feedback Loop and Maintenance

- **Performance Feedback**: Collecting performance data and user feedback.
- **Model Refinement**: Incorporating feedback and new data into the model training process for continuous improvement.

### Additional Considerations
- **Ethical and Privacy Considerations**: Ensuring the model and data usage comply with ethical standards and privacy regulations.
- **Security Measures**: Implementing security protocols to protect data and models from unauthorized access. -->

# Machine Learning - Software Development Life Cycle (ML-SDLC)

# Domain Knowledge

- **Run-up time** refers to the period leading up to a volcanic eruption. Specifically, it is the interval between when the first clear signs of unrest are detected (such as increased seismic activity, gas emissions, ground deformation) and the actual onset of the eruption. Run-up times can vary greatly among volcanoes and even from one eruption to another at the same volcano. They can range from very short periods of just a few hours or days where the volcano transitions quickly from a state of rest to eruption, to several weeks, months, or even years in cases where the volcanic activity slowly escalates. Monitoring the run-up time is crucial for volcanic hazard assessment and for issuing timely warnings to communities that may be at risk.



- Articles
    - [Observing eruptions of gas-rich compressible magmas from space](https://www.nature.com/articles/ncomms13744.pdf)
    - [The new science of volcanoes harnesses AI, satellites and gas sensors to forecast eruptions](https://www.nature.com/articles/d41586-020-01445-y)
    - [How AI and satellites could help predict volcanic eruptions](https://www.nature.com/articles/d41586-020-01445-y)
    - [A deep learning approach to detecting volcano deformation from satellite imagery using synthetic datasets](https://arxiv.org/pdf/1905.07286.pdf)
    - [Volcano inflation prior to an eruption: Numerical simulations based on a 1-D magma flow model in an open conduit](https://link.springer.com/content/pdf/10.5047/eps.2013.05.005.pdf)

# 1. Project Initialization

- Flags
- Constants
- Library Installations
- Library Imports
"""

# Flags
DEBUG_MODE = False
RESET_MODE = True

# Constants
VOLCANO_DATASET_URL = "https://drive.google.com/uc?export=download&id=1lNJLR0r8zxE-EU1H4f57OWLXr9agxBwb"  # Volcano dataset URL
V_IDS = range(1, 11)  # 10 volcanoes (as seen from the folder structure of the input dataset)

# Library Installations
if RESET_MODE:
    !pip install pandas seaborn tsfresh matplotlib lazypredict prophet plotly

# Library Imports
import glob
import re
from collections import OrderedDict

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from lazypredict.Supervised import LazyRegressor
from sklearn.metrics import mean_squared_error
from tsfresh import extract_features, select_features
from tsfresh.utilities.dataframe_functions import impute
from sklearn.model_selection import train_test_split

"""# Data Lifecycle

## Data Lifecycle: **Data Acquisition**
"""

# Download and Extract the Volcano Dataset
if RESET_MODE:
    !rm -f data.tar         && curl -L "{VOLCANO_DATASET_URL}" -o data.tar  # Downloads the dataset from Google Drive
    !rm -rf Volcano_Dataset && tar -xvf data.tar                            # Extracts `data.tar` to `Volcano_Dataset` directory
    !rm -rf data            && mv Volcano_Dataset data                      # Renames `Volcano_Dataset` to `data`

# Download and Extract the Volcano Dataset
if RESET_MODE:
    !rm -f data.tar         && curl -L "{VOLCANO_DATASET_URL}" -o data.tar  # Downloads the dataset from Google Drive
    !rm -rf Volcano_Dataset && tar -xvf data.tar                            # Extracts `data.tar` to `Volcano_Dataset` directory
    !rm -rf data            && mv Volcano_Dataset data                      # Renames `Volcano_Dataset` to `data`
# Library Installations
if RESET_MODE:
    !pip install pandas seaborn tsfresh matplotlib lazypredict prophet plotly

"""## Data Lifecycle: Data Transformation

Given the unstructured nature of the raw data, **transform** it into a **structured representation**, considering different strategies for handling fixed and variable-length observations.

This is a crucial step in the data lifecycle, as structured data is easier to analyze and model effectively. Effective data transformation and structuring are essential for successful machine learning modeling and analysis. The choice of data transformation strategy can significantly impact the performance and interpretability of machine learning models.

There are various approaches to accomplish this task:
- Construct forecasting models using a fixed-length observation.
- Consider feature engineering techniques to build a dataset from either variable or fixed-length observations.
"""

###########################################################
#   Volcano Observation File Paths and Count Extraction   #
###########################################################

file_paths = glob.glob('data/Volcano*/observation*.txt')
if DEBUG_MODE:
    print(f'\nObservation file paths: {file_paths}')

num_of_observations = len(file_paths)
if DEBUG_MODE:
    print(f'\nNumber of observations: {num_of_observations}')

############################################################
#   Volcano Observation ID Extraction (Helper Functions)   #
############################################################

volcano_observation_nums = lambda file_path: tuple(map(int, re.search(r'Volcano(\d+)/observation(\d+).txt', file_path).groups()))   # 'data/Volcano3/observation5.txt' -> (3, 5)
calculate_id = lambda volcano_num, observation_num: f"{volcano_num * 100 + observation_num:04d}"                                    # (3, 5) -> '0305'

# MAIN: observation id extraction
observation_id = lambda file_path: calculate_id(*volcano_observation_nums(file_path))                                               # 'data/Volcano3/observation5.txt' -> '0305'

###########################################################
#   Volcano Parameter Transformation (Helper Functions)   #
###########################################################

# line detection
line_has_parameters = lambda line: len(line.split(',')) == 2                                                            # 'v,0.25' -> True (parameter line detected)

# key/value string extraction
parameter_name_str = lambda line: line.split(',')[0].strip()                                                            # 'v,0.25' -> 'v'
parameter_value_str = lambda line: line.split(',')[1].strip()                                                           # 'v,0.25' -> '0.25'

# value type handling
scientific = lambda val: float(val.split('^')[0]) ** float(val.split('^')[1])                                           # '10^11.12' -> 1.012e+12
nano_radian = lambda val: float(val.strip('nrad')) * 10 ** -9                                                           # '3.51nrad' -> 3.51e-09

def handle_parameter_types(val_str):
    """
    Handles the different types of values in the parameter lines (scientific, nano-radian, float)
    :param val_str: value string
    :return: value
    """
    if '^' in val_str:
        return scientific(val_str)
    elif 'nrad' in val_str:
        return nano_radian(val_str)
    else:
        return float(val_str)

# parameter key/value extraction
parameter_name = lambda line: parameter_name_str(line)                                                                  # 'v,0.25' -> 'v'
parameter_value = lambda line: handle_parameter_types(parameter_value_str(line))                                        # 'v,0.25' -> 0.25

# MAIN: volcano parameters extraction
parameters = lambda lines: OrderedDict({parameter_name(line): parameter_value(line) for line in lines if line_has_parameters(line)})  # ['v,0.25\n', 'Patm,0.95\n', ...] -> {'v': 0.25, 'Patm': 0.95, ...}

#####################################################
#   Time Series Transformation (Helper Functions)   #
#####################################################

# time-series line-type handling
time_stamps = lambda _lines: _lines[-2].strip().split(',') if len(_lines) >= 2 else []
sensor_values = lambda _lines: _lines[-1].strip().split(',') if len(_lines) >= 2 else []

# MAIN: time series extraction
time_series = lambda _time_stamps, _sensor_values: OrderedDict(zip(map(int, _time_stamps), map(float, _sensor_values)))

#################################################
#   Sensor Data Statistics (Helper Functions)   #
#################################################

def sensor_readings_stats(sensor_data, eruption_val):
    """
    Computes statistics for the sensor data
    :param sensor_data:
    :param eruption_val:
    :return:  dictionary of statistics
    """
    stat_dict = {
        'mean': sensor_data.mean(),
        'median': sensor_data.median(),
        'mode': sensor_data.mode().iloc[0],
        'std': sensor_data.std(),
        'var': sensor_data.var(),
        'min': sensor_data.min(),
        'max': sensor_data.max(),
        'sum': sensor_data.sum(),
        'count': sensor_data.count(),
        'nunique': sensor_data.nunique(),
        'skew': sensor_data.skew(),
        'kurt': sensor_data.kurt(),
        'sem': sensor_data.sem(),
        'idxmax': sensor_data.idxmax(),
        'idxmin': sensor_data.idxmin(),
        'is_monotonic_increasing': sensor_data.is_monotonic_increasing,
        'is_monotonic_decreasing': sensor_data.is_monotonic_decreasing,
        'hasnans': sensor_data.hasnans,
        'empty': sensor_data.empty,
        'size': sensor_data.size,
        'ndim': sensor_data.ndim
    }
    return stat_dict

def time_stamp_to_date(time_stamp, date_0='2020-12-31'):
    """
    Converts the given time stamp to a date
    :param time_stamp: time stamp
    :param date_0: date at time stamp 0
    :return: date
    """
    return pd.to_datetime(date_0) - pd.to_timedelta(time_stamp, unit='D')

##############################################
#   Utility Functions for DataFrames (DFs)   #
##############################################

get_first_x_and_last_y_columns = lambda _df, _x, _y: pd.concat([_df.iloc[:, :_x], pd.DataFrame({'...': ['...'] * _df.shape[0]}), _df.iloc[:, -_y:]], axis=1)

def first_sensor_reading_col_index(_df):
    """
    What: Returns the index of the first sensor reading column
    How: It searches for the first column that contains a digit in its name
    """
    return [_df.columns.get_loc(col) for col in _df.columns if any(map(str.isdigit, str(col)))][0]


# p = lambda  _df: get_first_x_and_last_y_columns(_df, 15, -15).head() # alias for get_first_x_and_last_y_columns

def get_last_volcano_parameter_index(_df):
    """
    :param _df: dataframe
    :return:
    """
    if DEBUG_MODE:
        print(f'Last volcano parameter index: {first_sensor_reading_col_index(_df) - 1}')
    return first_sensor_reading_col_index(_df) - 1


def print_columns_in_range(_df, _start, _end):
    """
    :param _df: dataframe
    :param _start: start index
    :param _end: end index
    :return:
    """
    _d = _df.iloc[:, _start:_end]
    if DEBUG_MODE:
        print(f'\n\n\nColumns in the range {_start} to {_end} are:\n{_d.columns}')

def drop_column(_df, _col_name):
    _start = 0
    _end = get_last_volcano_parameter_index(_df) + 1

    print_columns_in_range(_df, _start, _end)
    if DEBUG_MODE:
        print(f'{_col_name} column is {"in" if _col_name in _df.columns else "not in"} the dataframe.')

    if _col_name in _df.columns:
        _df.drop(_col_name, axis=1, inplace=True)
        if DEBUG_MODE:
            print(f'{_col_name} column dropped.')

    print_columns_in_range(_df, _start, _end)


def drop_constant_columns_in_range(_df, _start, _end):
    """
    Drops constant columns in the given range
    :param _df: dataframe
    :param _start: start index
    :param _end: end index
    :return:
    """
    # drop_constant_columns_in_range = lambda _df, _start, _end: _df.drop(columns=_df.iloc[:, _start:_end].columns[_df.iloc[:, _start:_end].nunique() == 1])
    print_columns_in_range(_df, _start, _end)
    if DEBUG_MODE:
        print(f'\n\n\nCONSTANT columns ({_start}, {_end}): {_df.iloc[:, _start:_end].columns[_df.iloc[:, _start:_end].nunique() == 1].tolist()}')
    _df.drop(columns=_df.iloc[:, _start:_end].columns[_df.iloc[:, _start:_end].nunique() == 1], inplace=True)
    if DEBUG_MODE:
        print(f'\n\n\nCONSTANT columns dropped.')
    print_columns_in_range(_df, _start, _end+5)

def get_first_not_null_sensor_reading_col_index(_df, _o_idx):
    """
    What: Returns the index (not the name) of the first non-null sensor reading column for a given observation
    How: It searches for the first non-null value in the sensor reading columns for the given observation

    :param _df: dataframe
    :param _o_idx: observation index
    """
    # return _df.iloc[_o_idx, first_sensor_reading_col_index(_df):].first_valid_index()
    # the above doesn't work because for a single row in df, the column name becomes the index (i.e. the column name is returned)
    _sensor_readings = _df.iloc[_o_idx, first_sensor_reading_col_index(_df):]
    column_name = _sensor_readings.first_valid_index()
    _idx = _df.columns.get_loc(column_name)

    # verify for observation 0
    if DEBUG_MODE:
        print(f'\n---------------------------------------------------------')
        print(f'\nID: {_df.iloc[_o_idx, 0]}')
        print(f'\nBEFORE: {_idx - 1}\ndf[{_o_idx}, {_idx - 1}] (column name = {column_name - 1}): {_df.iloc[_o_idx, _idx - 1]}')
        print(f'\nINDEX BEING RETURNED: {_idx}\ndf[{_o_idx}, {_idx}] (column name = {column_name}): {_df.iloc[_o_idx, _idx]}')
        print(f'\nAFTER: {_idx + 1}\ndf[{_o_idx}, {_idx + 1}] (column name = {column_name + 1}): {_df.iloc[_o_idx, _idx + 1]}')

    return _idx

if DEBUG_MODE:
    assert get_first_not_null_sensor_reading_col_index(df, 0) == 620  # 1 (id) + 6 (v_params) + 613 (number of Nan before -1052, the first non-null column name) = 620
    assert get_first_not_null_sensor_reading_col_index(df, 1) == 175  # 1 (id) + 6 (v_params) + 168 (number of Nan before -1497, the first non-null column name) = 175
    assert get_first_not_null_sensor_reading_col_index(df, 2) == 437  # 1 (id) + 6 (v_params) + 430 (number of Nan before -1234, the first non-null column name) = 437
    assert get_first_not_null_sensor_reading_col_index(df, 3) == 633  # 1 (id) + 6 (v_params) + 626 (number of Nan before -1039, the first non-null column name) = 633
    assert get_first_not_null_sensor_reading_col_index(df, 4) == 508  # 1 (id) + 6 (v_params) + 501 (number of Nan before -1164, the first non-null column name) = 508
    assert get_first_not_null_sensor_reading_col_index(df, 5) == 432  # 1 (id) + 6 (v_params) + 425 (number of Nan before -1240, the first non-null column name) = 432
    assert get_first_not_null_sensor_reading_col_index(df, 6) == 288  # 1 (id) + 6 (v_params) + 281 (number of Nan before -1384, the first non-null column name) = 288
    assert get_first_not_null_sensor_reading_col_index(df, 7) == 331  # 1 (id) + 6 (v_params) + 324 (number of Nan before -1341, the first non-null column name) = 331
    assert get_first_not_null_sensor_reading_col_index(df, 8) == 323  # 1 (id) + 6 (v_params) + 316 (number of Nan before -1349, the first non-null column name) = 323
    assert get_first_not_null_sensor_reading_col_index(df, 9) == 453  # 1 (id) + 6 (v_params) + 446 (number of Nan before -1219, the first non-null column name) = 453
    assert get_first_not_null_sensor_reading_col_index(df, 10) == 327  # 1 (id) + 6 (v_params) + 320 (number of Nan before -1345, the first non-null column name) = 327
    assert get_first_not_null_sensor_reading_col_index(df, 126) == 7  # 1 (id) + 6 (v_params) + 0 (number of Nan before -1483, the first non-null column name) = 7

def check_missing_sensor_readings_in_middle(_df):
    """
    What: Checks for missing sensor readings in all observations
    How: It counts the number of missing sensor readings in all observations
    """
    return [_df.iloc[_o_idx, get_first_not_null_sensor_reading_col_index(_df, _o_idx):].isnull().sum() for _o_idx in range(_df.shape[0])]

if DEBUG_MODE:
    assert check_missing_sensor_readings_in_middle(df) == [0] * df.shape[0]

#################################################
#   Sensor Data Statistics (Helper Functions)   #
#################################################

def sensor_readings_stats(sensor_data):
    """
    Computes statistics for the sensor data
    :param sensor_data:
    :param eruption_val:
    :return:  dictionary of statistics
    """
    stat_dict = {
        'mean': sensor_data.mean(),
        'median': sensor_data.median(),
        'mode': sensor_data.mode().iloc[0],
        'std': sensor_data.std(),
        'var': sensor_data.var(),
        'min': sensor_data.min(),
        'max': sensor_data.max(),
        'sum': sensor_data.sum(),
        'count': sensor_data.count(),
        'nunique': sensor_data.nunique(),
        'skew': sensor_data.skew(),
        'kurt': sensor_data.kurt(),
        'sem': sensor_data.sem(),
        'idxmax': sensor_data.idxmax(),
        'idxmin': sensor_data.idxmin(),
        'is_monotonic_increasing': sensor_data.is_monotonic_increasing,
        'is_monotonic_decreasing': sensor_data.is_monotonic_decreasing,
        'hasnans': sensor_data.hasnans,
        'empty': sensor_data.empty,
        'size': sensor_data.size,
        'ndim': sensor_data.ndim
    }
    return stat_dict


def p(_df):
    """
    Created for speed (for when you run all cells). It drops everything but first 15 and last 15 columns of the dataframe and return the head
    :param _df: dataframe
    :return:
    """
    _d = _df.copy()
    # drop all but first 15 and last 15 columns
    _d = _d.iloc[:, :15]
    _d = pd.concat([_d, pd.DataFrame({'...': ['...'] * _d.shape[0]}), _df.iloc[:, -15:]], axis=1)
    return _d.head()

####################################
#   DataFrame (Helper Functions)   #
####################################

observations = [] # list of dictionaries (each dictionary is an observation) in wide format

def get_dataframe_for_entire_data():
    """
    Creates the DataFrame from the given file paths

    :return: DataFrame
    """

    global observations

    for file_path in file_paths:
        with open(file_path, 'r') as file:
            lines = file.readlines()

        observations.append({
            'id': observation_id(file_path),
            **parameters(lines),
            **time_series(time_stamps(lines), sensor_values(lines))
        })

    # sort by id (first by int(volcano_id) and then by int(observation_id))
    observations = sorted(observations, key=lambda obs: obs['id'])

    # create dataframe
    _df = pd.DataFrame(observations)

    # sort sensor_readings by their timestamps
    sr1_index = first_sensor_reading_col_index(_df)
    _df = _df[_df.columns[:sr1_index].tolist() + sorted(_df.columns[sr1_index:].tolist(), key=lambda col: int(col))]

    return _df

"""## Data Lifecycle > Data Preparation > Data Validation

Validation of the input data to ensure it follows the specified format.
"""

#############################################################################
#   Unit Tests (Helped immensely in EDA and when major changes were made)   #
#############################################################################

def observation_id_extraction_unit_test():
    print('\nRunning observation_id_extraction_unit_test...')

    assert volcano_observation_nums('data/Volcano3/observation5.txt') == (3, 5)
    assert calculate_id(3, 5) == '0305'
    assert observation_id('data/Volcano3/observation5.txt') == '0305'

    print('observation_id_extraction_unit_test passed!')


def parameter_transformation_unit_test():
    print('\nRunning parameter_transformation_unit_test...')

    assert line_has_parameters('v,0.25\n') == True
    assert parameter_name_str('v,0.25') == 'v'
    assert parameter_value_str('v,0.25') == '0.25'
    assert scientific('10^11.12') == 131825673855.64047
    assert nano_radian('3.51nrad') == 3.51e-09
    assert handle_parameter_types('10^11.12') == 131825673855.64047
    assert handle_parameter_types('3.51nrad') == 3.51e-09
    assert handle_parameter_types('0.25') == 0.25
    assert parameter_name('v,0.25') == 'v'
    assert parameter_value('v,0.25') == 0.25
    assert parameter_value('tilt_erupt,3.1271nrad') == 3.1271e-09
    assert parameters(['v,0.25\n', 'Patm,0.95\n', 'g,9.8\n', 'r,0.25\n', 'G,6.674e-11\n', 'rho,1.225\n', 'mu,0.000018\n', 'rc,0.25\n', 'M,5.972e24\n', 'sigma,5.67e-8\n', 'tilt_erupt,3.1271nrad\n']) == OrderedDict([('v', 0.25), ('Patm', 0.95), ('g', 9.8), ('r', 0.25), ('G', 6.674e-11), ('rho', 1.225), ('mu', 0.000018), ('rc', 0.25), ('M', 5.972e24), ('sigma', 5.67e-8), ('tilt_erupt', 3.1271e-09)])

    print('parameter_transformation_unit_test passed!')

def time_series_unit_test():
    print('\nRunning time_series_unit_test...')

    assert time_stamps(['-1489,-1488,-1487,-2,-1,0', '-2.2677e-10,4.48801e-11,-5.38847155851477e-11']) == ['-1489', '-1488', '-1487', '-2', '-1', '0']
    assert sensor_values(['-1489,-1488,-1487', '-2.2677e-10,4.48801e-11,-5.38847155851477e-11']) == ['-2.2677e-10', '4.48801e-11', '-5.38847155851477e-11']
    assert time_series(['-1489', '-1488', '-1487'], ['0.25', '0.26', '0.27']) == OrderedDict({-1489: 0.25, -1488: 0.26, -1487: 0.27})
    assert time_series([], []) == OrderedDict()
    assert time_series(['-1489', '-1488', '-1487'], []) == OrderedDict()
    assert time_series([], ['0.25', '0.26', '0.27']) == OrderedDict()
    assert time_series(['-1489', '-1488', '-1487'], ['0.25', '0.26']) == OrderedDict({-1489: 0.25, -1488: 0.26})
    assert time_series(['-1489', '-1488'], ['0.25', '0.26', '0.27']) == OrderedDict({-1489: 0.25, -1488: 0.26})
    assert time_series(['-1489'], ['0.25']) == OrderedDict({-1489: 0.25})
    assert time_series(['-1489', '-1488', '-1487', '-2', '-1', '0'],
                       ['-2.2677e-10', '4.48801e-11', '-5.38847155851477e-11', '-2.2677e-10', '4.48801e-11', '-5.38847155851477e-11', '-2.2677e-10', '4.48801e-11', '-5.38847155851477e-11']
                       ) == OrderedDict([(int('-1489'), -2.2677e-10), (int('-1488'), 4.48801e-11), (int('-1487'), -5.38847155851477e-11), (-2, -2.2677e-10), (-1, 4.48801e-11), (0, -5.38847155851477e-11)])

    print('time_series_unit_test passed!')

def get_dataframes_for_entire_data_unit_test():
    print('\nRunning get_dataframes_for_entire_data_unit_test...')

    wide_df, long_df = get_dataframe_for_entire_data()

    assert wide_df.shape == (189, 1678)
    assert drop_constant_columns_in_range(wide_df, 0, 5).shape == (189, 1674)
    assert get_first_x_and_last_y_columns(wide_df, 5, 3).shape == (189, 9)

    assert long_df.shape == (242644, 14)
    assert get_first_x_and_last_y_columns(long_df, 5, 3).shape == (242644, 9)

    print('get_dataframes_for_entire_data_unit_test passed!')

if DEBUG_MODE:
    observation_id_extraction_unit_test()
    observation_id_extraction_unit_test()
    parameter_transformation_unit_test()
    time_series_unit_test()
    get_dataframes_for_entire_data_unit_test()

df = get_dataframe_for_entire_data()

"""### Data Transformation Notes

- id: volcano id with 4 digits.
    - First two digits = Volcano number.
    - Last two digits = Observation number.
    - Example: `Volcano1/observation1.txt` has id `0101`.

## Initial Data Exploration
"""

# Displaying the overall structure of the dataset
print("Dataset Shape:", df.shape)
print("\n")

# Displaying data types of columns
print("Data Types of Columns:")
print(df.dtypes)
print("\n")

# Summary statistics for numerical features
print("Summary Statistics for Numerical Features:")
df.describe()

# Checking for missing values in each column
missing_values = df.isnull().sum()
print("Missing Values in Each Column:")
missing_values[missing_values > 0]

# Checking for duplicate rows
duplicate_rows = df.duplicated().sum()
print("Number of Duplicate Rows:", duplicate_rows)

if duplicate_rows > 0:
    # Displaying duplicate rows
    print("Duplicate Rows:")
    print(df[df.duplicated()])

"""## Standard Deviation Visualizations"""

# list of all ids
ids = df['id'].tolist()

# list of all volcano ids from ids
v_ids = list(set([int(i[:2]) for i in ids]))

def plot_sensor_readings_and_stats(df, v_ids, o_num):
    # Sort to ensure we have a deterministic order
    v_ids = sorted(v_ids)

    total_plots = len(v_ids) * o_num * 2  # 2 plots (sensor readings and stats) per observation
    fig, axs = plt.subplots(total_plots, 1, figsize=(20, 10 * total_plots))

    plot_index = 0  # Keep track of which subplot we're working on

    for v_id in v_ids:
        o_ids = sorted(set([int(i[2:]) for i in ids if i.startswith(f'{v_id:02}')]))

        for o_id in o_ids[:o_num]:  # Process only the first o_num observations
            fsri = get_first_not_null_sensor_reading_col_index(df, o_id)
            sensor_readings = df.iloc[o_id-1, -500:]

            window_sizes = range(1, 1001)  # Define window sizes
            avg_std_devs = [sensor_readings.rolling(window=ws).std().mean() for ws in window_sizes]

            # Plot sensor readings
            axs[plot_index].scatter(sensor_readings.index, sensor_readings, label=f'Obs {o_id}')
            axs[plot_index].scatter(sensor_readings.index[-1], sensor_readings[-1], color='red', label='Eruption Value')
            axs[plot_index].set_title(f'Volcano {v_id} - Observation {o_id} - Sensor Readings')
            axs[plot_index].set_xlabel('Timestep')
            axs[plot_index].set_ylabel('Sensor Reading')
            axs[plot_index].legend()
            plot_index += 1  # Move to the next plot for the next iteration

            # Plot rolling window stats
            axs[plot_index].plot(window_sizes, avg_std_devs, label=f'Obs {o_id}', marker='o')
            axs[plot_index].set_title(f'Volcano {v_id} - Observation {o_id} - Avg Rolling Window Std Dev')
            axs[plot_index].set_xlabel('Window Size')
            axs[plot_index].set_ylabel('Average Std Dev')
            axs[plot_index].legend()

            plot_index += 1  # Ready for the next pair of plots

    plt.tight_layout()
    plt.show()

plot_sensor_readings_and_stats(df, v_ids[:1], 3)

"""### Basic Information

Extract basic information about the dataset, such as:

1. shape: number of rows and columns.
2. head: first few rows of the dataframe.
3. info: column names, non-null counts, and data types.
"""

df.shape

p(df.head())

df.info()

"""### Basic Information - Results

1. The dataset contains **189 observations** and **1678 columns**.
2. The first 12 columns represent the **volcano parameters**.
3. The remaining columns represent the **time series data**.
4. The dataset contains **no missing values** for the volcano parameters columns.

### Summary Statistics
"""

df.describe()

"""### Summary Statistics - Results

- The numerical columns in the dataset have a variety of means and standard deviations, which suggests ==**the data spans over different scales**==.

### Missing Values

#### Missing Values - Volcano Parameters
"""

missing_v_param_values = df.iloc[:, 1:get_last_volcano_parameter_index(df) + 1].isnull().sum()
if missing_v_param_values.sum() == 0:
    print('No missing values in the volcano parameters.')

"""#### Missing Values - Sensor Readings"""

missing_values = df.isnull().sum()
if missing_values[missing_values > 0].shape[0] == 0:
    print('No missing values in the sensor readings.')
else:
    print('Missing values in the sensor readings:')

    pd.set_option('display.max_rows', None)
    print(missing_values[missing_values > 0])
    pd.reset_option('display.max_rows')

"""#### Missing Values - Sensor Readings - After First Non-Null Value"""

if check_missing_sensor_readings_in_middle(df) == [0] * df.shape[0]:
    print('No missing sensor readings in any observation.')

"""### Missing Values - Results

- The time-step columns do not have missing values for the last ten steps (-10 to 0), indicating these correspond to ==**the moments leading to the eruption and are consistently recorded for all observations**==.
- However, there are missing values starting from the **-1003.0** column all the way to **-1665.0**, increasing as we go further back in time steps before the eruption, with up to 186 missing values in some columns.

## (Drop) title_erupt

As per the instructions, the `tilt_erupt` value specified in the volcano parameters is to be ignored, and the last sensor reading in the time series is to be used as the tilt-erupt value.
"""

drop_column(df, 'tilt_erupt')

"""## (Drop) Constant Columns"""

drop_constant_columns_in_range(df, 0, get_last_volcano_parameter_index(df) + 1)

p(df)

"""## (Insert) Number of Sensor Readings Column"""

if 'num_sensor_readings' not in df.columns:
    _col_loc = get_last_volcano_parameter_index(df) + 1
    df.insert(_col_loc, 'num_sensor_readings', df.iloc[:, first_sensor_reading_col_index(df):].count(axis=1))

p(df)

"""# Interesting observations

- higher 'rc' value seems to be directly proportional to the less chances of dip (pre-eruption).

# Initial Visualization

## Volcano Parameters Visualization
"""

def plot_distribution(_df, _col, _legend_loc='lower right'):
    """
    Plots sensor readings and a volcano parameter for all observations with consistent colors for each volcano
    :param _df: DataFrame
    :param _col: volcano parameter name
    :param _legend_loc: legend location
    :return: None
    """
    _df = _df.sort_values(by=_col, ascending=True)
    colors = ['red', 'green', 'blue', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']

    # plot for each volcano
    plt.figure(figsize=(20, 40))
    for v_id in V_IDS:
        v_df = _df[_df['id'].str.startswith(f'{v_id:02}')]
        plt.subplot(5, 2, v_id)
        plt.xticks(range(v_df.shape[0]), [str(int(i[-2:])) for i in v_df['id']])
        plt.scatter(v_df['id'], v_df[_col], color=colors[v_id - 1])  # Color is selected based on the volcano ID
        plt.title(f'Volcano {v_id}')
        plt.xlabel('Observation ID')
        plt.ylabel(_col)

    # overall plot
    plt.figure(figsize=(50, 10))
    for v_id in V_IDS:
        v_df = _df[_df['id'].str.startswith(f'{v_id:02}')]
        plt.scatter(v_df['id'], v_df[_col], color=colors[v_id - 1], label=f'Volcano {v_id}')  # Consistent color
    plt.xticks(range(_df.shape[0]), [str(int(i[-2:])) for i in _df['id']])
    plt.title(f'{_col} vs Observation ID')
    plt.xlabel('Observation ID')
    plt.ylabel(_col)
    plt.legend(loc=_legend_loc, ncol=10, prop={'size': 20})
    plt.show()

plot_distribution(df, 'num_sensor_readings', 'upper center')

plot_distribution(df, 'G', 'upper right')

plot_distribution(df, 'rho', 'upper right')

plot_distribution(df, 'mu', 'upper right')

plot_distribution(df, 'rc', 'upper right')

plot_distribution(df, 'M', 'upper right')

plot_distribution(df, 'sigma', 'upper right')

"""## Interesting Observations

- 'G', 'rho', 'mu', 'rc', change for every volcano but remain constant for all observations of a volcano.
- 'm' and 'sigma' change for every observation.
- 'sigma always has 3 values for every volcano: 0.1, 0.25 and 0.5.

## Volcano Sensor Readings Visualizations
"""

SHOW_ONLY_FIRST_VOLCANO_OBSERVATIONS = True

if SHOW_ONLY_FIRST_VOLCANO_OBSERVATIONS:
    # list of all ids where ids start with '01'
    vo_ids = [i for i in df['id'].tolist() if i.startswith('01')]
else:
    vo_ids = df['id'].tolist()
# list of all volcano ids from ids
v_ids = list(set([int(i[:2]) for i in vo_ids]))
for v_id in v_ids:
    # list of all observation ids for the current volcano
    current_volcano_vo_ids = list(set([int(i[2:]) for i in vo_ids if i.startswith(f'{v_id:02}')]))
    # plot for all observations for the current volcano (all observations in 1 column, figsize calculation based on number of observations)
    plt.figure(figsize=(20, 7 * len(current_volcano_vo_ids)))

    # outer plot title
    plt.suptitle(f'Volcano {v_id} Sensor Readings', fontsize=20)

    for vo_id in current_volcano_vo_ids:

        v_num = f'{v_id:02}'
        o_num = f'{vo_id:02}'

        # get the row corresponding to the current volcano and observation
        current_row = df[df['id'] == f'{v_num}{o_num}'].iloc[0]

        v_param = '\n'.join([f'{k} = {v:.2e}' for k, v in current_row[['G', 'rho', 'mu', 'rc']].items()])
        o_param = '\n'.join([f'{k} = {v:.2e}' for k, v in current_row[['M', 'sigma']].items()])
        # plot for each observation
        plt.subplot(len(current_volcano_vo_ids), 1, vo_id)

        # plot background color based on sigma value (0.1: lightgreen, 0.25: lightyellow, 0.5: lightrcoral)
        sigma_val = current_row['sigma']
        plt.gca().set_facecolor('lightgreen' if sigma_val == 0.1 else 'lightyellow' if sigma_val == 0.25 else 'lightcoral')

        # textbox in top left of plot to mention M, and sigma values
        plt.text(0.05, 0.95, f'{v_param}\n\n\n{o_param}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')

        # scatter plot of sensor readings
        plt.scatter(df.columns[first_sensor_reading_col_index(df):-1], current_row[first_sensor_reading_col_index(df):-1], color='blue')
        # highlight the eruption value
        plt.scatter(df.columns[-1], current_row[-1], color='red')
        # line plot of rolling mean (window=5)
        plt.plot(current_row[first_sensor_reading_col_index(df):-1].rolling(window=5).mean(), color='green')
        # line plot of rolling mean (window=10)
        plt.plot(current_row[first_sensor_reading_col_index(df):-1].rolling(window=10).mean(), color='orange')
        plt.title(f'Observation {o_num}')
        plt.xlabel('Timesteps')
        plt.ylabel('Sensor Readings')

    plt.show()

"""## First & Second Differential - Manual experimentation"""

# WORK IN PROGRESS

# def plot_differentials(df):
#     """
#     Plots the first and second differentials for each observation in the DataFrame.
#
#     Args:
#         df (pandas.DataFrame): The input DataFrame containing the data.
#         v_ids (list): A list of unique vehicle IDs.
#         o_ids (list): A list of unique observation IDs.
#
#     Returns:
#         None
#     """
#     cols = df.columns[df.columns >= start_idx]
#     xs = cols.astype(int).tolist()
#     ys = filtered_df.iloc[0, start_idx:end_idx].astype(float).tolist()
#
#     # Calculate first and second differentials
#     first_diff = [ys[i] - ys[i - 1] for i in range(1, len(ys))]
#     second_diff = [first_diff[i] - first_diff[i - 1] for i in range(1, len(first_diff))]
#
#     # Plot the differentials
#     fig, ax = plt.subplots(figsize=(10, 6))
#     ax.plot(xs[1:], first_diff, label='First Differential')
#     ax.plot(xs[2:], second_diff, label='Second Differential')
#     ax.set_title(f'Differentials for vo_id: {vo_id}')
#     ax.set_xlabel('Time Steps')
#     ax.set_ylabel('Differential Values')
#     ax.legend()
#     plt.show()
#
# plot_differentials(df)
#
# # for v_id in V_IDS:
# #     o_ids = sorted(set([int(i[2:]) for i in df['id'].tolist() if i.startswith(f'{v_id:02}')]))
# #
# #     for o_id in o_ids:
# #
# #         vo_id = f'{v_id:02}{o_id:02}'
# #
# #         start_idx, end_idx = df['sr_start_idx'][df['id'] == vo_id].values[0], 0
# #         mask = (df['id'] == vo_id) & (df.columns >= start_idx)
# #         xs = [int(c) for c in df.columns[start_idx:]]  # Time steps
# #         ys = df.loc[mask].iloc[0].astype(float).tolist()  # we need iloc[0] because we are using mask and
# #
# #         break
# #     break
#
#     #     xs = [int(c) for c in df.columns[start_idx:]]  # Time steps
#     #     ys = df.loc[df['id'] == vo_id, start_idx:].iloc[0].astype(float).tolist()  # Sensor readings
#     #     first_diff = np.diff(ys)
#     #     second_diff = np.diff(first_diff)
#     #
#     #     # Plot the first differential
#     #     plt.figure(figsize=(40, 20))
#     #     # plot the sensor readings
#     #     plt.scatter(xs, ys, color='red')
#     #     plt.plot(xs, ys, color='red')
#     #     # plot the first differential
#     #     plt.plot(xs[1:], first_diff, color='green')
#     #     plt.plot(xs[2:], second_diff, color='blue')
#     #
#     #     plt.title(f'Volcano {v_id} - Observation {o_id} - First Differential of Sensor Readings')
#     #     plt.xlabel('Timesteps')
#     #     plt.ylabel('First Differential of Sensor Readings')
#     #     plt.show()
#     #     break
#     # break
#
#     # for o_id in range(1, v_df.shape[0] + 1):
#     #     o_df = v_df[v_df['id'].str.endswith(f'{o_id:02}')]
#     #
#     #     # Calculate first differential
#     #     start_idx =
#     #     end_idx = 0
#     #     xs = [int(c) for c in o_df.columns[start_idx:]]  # Time steps
#     #     ys = [float(y) for y in o_df.iloc[0, start_idx:]]  # Sensor readings
#     #     first_diff = np.diff(ys)
#     #     second_diff = np.diff(first_diff)
#     #
#     #     # Plot the first differential
#     #     plt.figure(figsize=(40, 20))
#     #     # plot the sensor readings
#     #     plt.scatter(xs, ys, color='red')
#     #     plt.plot(xs, ys, color='red')
#     #     # plot the first differential
#     #     plt.plot(xs[1:], first_diff, color='green')
#     #     plt.plot(xs[2:], second_diff, color='blue')
#     #
#     #     plt.title(f'Volcano {v_id} - Observation {o_id} - First Differential of Sensor Readings')
#     #     plt.xlabel('Timesteps')
#     #     plt.ylabel('First Differential of Sensor Readings')
#     #     plt.show()
#     #     break
#     # break

"""## Prophet experimentation


"""

from prophet.plot import plot_plotly
from prophet.plot import add_changepoints_to_plot
from prophet import Prophet

prophet_df = df.copy()

df_idx = -1


for v_id in V_IDS:
    v_df = prophet_df[prophet_df['id'].str.startswith(f'{v_id:02}')]

    for o_id in range(1, v_df.shape[0] + 1):

        df_idx += 1

        o_df = v_df[v_df['id'].str.endswith(f'{o_id:02}')]

        # Prepare data
        start_idx = 8
        end_idx = 0

        # Time steps
        xs = [int(c) for c in o_df.columns[start_idx:]]  # Time steps
        date_offset = xs[0]
        # [-1052, -1051, -1050, ..., -2, -1, 0] list(int) -> [... 2020-12-30, 2020-12-31] list(pd.Timestamp) 0 = 2020-12-31 and backtracking in time for earlier indices
        xs = [(pd.Timestamp('2020-12-31') - pd.DateOffset(days=abs(c))).date() for c in xs]

        # Sensor readings
        ys = [float(y) for y in o_df.iloc[0, start_idx:]]  # Sensor readings

        # Create a DataFrame for Prophet
        data = pd.DataFrame({'ds': xs, 'y': ys})

        # Fit the model
        model = Prophet()
        model.fit(data)

        # Make future predictions
        # future = model.make_future_dataframe(periods=1, include_history=True)
        future = model.make_future_dataframe(periods=1)
        forecast = model.predict(future)


        model.plot_components(forecast)

        fig = model.plot(forecast)
        a = add_changepoints_to_plot(fig.gca(), model, forecast)


        # ##### Multiplicative Seasonality Model
        #
        # future = model.make_future_dataframe(50, freq='MS')
        # forecast = model.predict(future)
        # fig = model.plot(forecast)
        #
        #
        # m2 = Prophet(seasonality_mode='multiplicative')
        # m2.fit(data)
        # forecast2 = m2.predict(future)
        # fig = m2.plot(forecast2)


        break  # Remove this break point if you want to see all observations of all volcanoes
    # break

"""### Interesting Observations

- it would be good to note the impact of outlier removal on the changepoint detection.
- change points can be used for 3-phase detection (normal, pre-spike-dip, spike)
    - Great future avenue for ensemble learning.
"""

# Sensor Readings only
sr = df.copy()
sr.drop(sr.columns[1:get_last_volcano_parameter_index(sr)+1], axis=1, inplace=True)

# normalize sr

sr_min = sr.iloc[:, 1:].min().min()
sr_max = sr.iloc[:, 1:].max().max()

sr.iloc[:, 1:] = (sr.iloc[:, 1:] - sr_min) / (sr_max - sr_min)

# drop all rows but the first 1 (FOR TESTING PURPOSES)
# sr = sr.head(50)

def create_sliding_windows(data, window_size, step_size):
    num_windows = (data.shape[0] - window_size) // step_size + 1
    remainder = (data.shape[0] - window_size) % step_size

    if remainder != 0:
        num_windows += 1
        pad_size = step_size - remainder
        padded_data = np.pad(data, (0, pad_size), mode='constant', constant_values=np.nan)
    else:
        padded_data = data

    shape = (num_windows, window_size)
    strides = (padded_data.strides[0] * step_size,) + padded_data.strides
    windows = np.lib.stride_tricks.as_strided(padded_data, shape=shape, strides=strides)

    return windows

# Enum for Prediction Task ('Time to Eruption' or 'Eruption Sensor Reading')
class PredictionTask:
    TIME_TO_ERUPTION = 1
    ERUPTION_SENSOR_READING = 2

def generate_sliding_window_dataframe(_df_type, _sr, _window_size, _step_size):

    sr_last_col_idx = len(_sr.columns) - 1

    window_data = []
    for obs_id in _sr['id']:

        start_idx = 1  # to prevent the id column from being included in the sensor readings
        end_idx = sr_last_col_idx + 1 \
            if _df_type == PredictionTask.TIME_TO_ERUPTION \
            else -1

        obs_data = _sr[_sr['id'] == obs_id].iloc[:, start_idx:end_idx].values[0]               # sensor readings for the observation
        obs_data = obs_data[~np.isnan(obs_data)]                                # remove NaNs
        windows = create_sliding_windows(obs_data, _window_size, _step_size)    # create sliding windows with specified step size
        len_windows = len(windows)

        if _df_type == PredictionTask.TIME_TO_ERUPTION:
            # time to eruption for each window
            target_column = np.arange(0, (len_windows * _step_size), _step_size)[::-1]
        else:
            # eruption sensor reading for each window (same for all windows of an observation: sr's last column)
            target_column = np.full(len_windows, _sr[_sr['id'] == obs_id].iloc[0, -1])

        # ADDITIONAL: For
        # # Add all volcano parameters to v_param_df for each window (since they are the same for an observation, we can just add them once based on len_windows)
        # v_param_repeat = len_windows
        # for i in range(v_param_repeat):
        #     v_param_df = v_param_df.append(df[df['id'] == obs_id].iloc[:, 1:get_last_volcano_parameter_index(df)+1], ignore_index=True)

        window_id = [f'{obs_id}{i:05}' for i in range(len(windows))]                        # window id (0101421: volcano: 1, observation: 1, window #: 421)
        temp_data = np.column_stack((window_id, windows, target_column))                    # combine window_id, windows, and target_column
        window_data.append(temp_data)                                                       # append to window_data list

    window_data = np.concatenate(window_data, axis=0)                                       # convert list of arrays to a single array (stacking)

    target_column_name = 'time_to_erupt' \
        if _df_type == PredictionTask.TIME_TO_ERUPTION \
        else 'erupt_sr'

    columns = ['window_id'] + [str(i) for i in range(_window_size)] + [target_column_name]  # column names for the window dataframe
    _window_df = pd.DataFrame(window_data, columns=columns)                                 # window dataframe
    return _window_df

def generate_X_y(_df_type, _window_df):
    target_column_name = 'time_to_erupt' \
        if _df_type == PredictionTask.TIME_TO_ERUPTION \
        else 'erupt_sr'

    melted_df = pd.melt(
        frame=window_df,
        id_vars=['window_id', target_column_name],
        var_name='time',
        value_name='sensor_reading')    # melt the window dataframe

    melted_df['sensor_reading'] = melted_df['sensor_reading'].replace('nan', np.nan)
    melted_df = melted_df.dropna(subset=['sensor_reading'])
    melted_df['time'] = melted_df['time'].astype(int)                                                                       # typecast time-steps to int

    # sort by window_id and then by time
    melted_df.sort_values(by=['window_id', 'time'], inplace=True)       # sort by window_id and time
    melted_df.reset_index(drop=True, inplace=True)                      # reset index

    # rearrange columns: window_id, time, sensor_reading, time_to_erupt
    melted_df = melted_df[['window_id', 'time', 'sensor_reading', target_column_name]] # rearrange columns

    X = melted_df[['window_id', 'time', 'sensor_reading']]      # features
    X['time'] = X['time'].astype(int)                           # typecast time-steps to int
    X['sensor_reading'] = X['sensor_reading'].astype(float)     # typecast sensor readings to float

    y = melted_df[['window_id', target_column_name]]            # target

    target_column_data_type = int \
        if _df_type == PredictionTask.TIME_TO_ERUPTION \
        else float
    y[target_column_name] = y[target_column_name].astype(target_column_data_type)  # typecast target to int or float
    y.set_index('window_id', drop=True, inplace=True)           # set window_id as index
    y = y.squeeze()                                             # convert to series
    y = y[~y.index.duplicated(keep='first')]                    # drop duplicates

    return X, y

"""# Feature Extraction"""

def extract_all_features(X):
    _extracted_features = extract_features(X, column_id='window_id', column_sort='time')

    # drop features that have any NaN values (using this approach in lieu of impute to retain as much original data as possible)
    _extracted_features = _extracted_features.dropna(axis=1)

    return _extracted_features

"""# Feature Selection"""

def select_relevant_features(_extracted_features, y):
    _selected_features = select_features(_extracted_features, y, ml_task='regression')
    _selected_features.to_csv(f'time_to_erupt_win_{window_size}_step_{step_size}.csv', index=False)
    return _selected_features

"""# Automated Model Selection"""

def select_model(_pred_task_name, X_train, X_test, y_train, y_test):

    pred_task_name = 'time_to_erupt' \
        if _pred_task_name == PredictionTask.TIME_TO_ERUPTION \
        else 'erupt_sr'

    def MSE(y_true, y_pred):
        return mean_squared_error(y_true, y_pred)

    reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=MSE)
    models, predictions = reg.fit(X_train, X_test, y_train, y_test)

    # sort models by adjusted r2
    models.sort_values(by='Adjusted R-Squared', ascending=False, inplace=True)

    # Drop Time Taken Column
    report = models.drop(columns=['Time Taken'])

    # sort report
    report = report.sort_values(by='Adjusted R-Squared', ascending=False)


    # save report in csv (increment number in file name if file exists)
    i = 0
    while True:
        try:
            report.to_csv(f'{pred_task_name}_report_{window_size}_{step_size}_{i}.csv')
            break
        except FileExistsError:
            i += 1
        finally:
            pass

    return report

"""# Machine Learning Task: Predict Time To Erupt


I have experimented with several window sizes. This notebook aims to help real-world scenarios where depending on how much data we have from a real volcano.
"""

# Time to Eruption
window_sizes = [10, 50, 100, 300, 500]
step_sizes = [5, 25, 50, 100, 300]

for i in range(len(window_sizes)):
    window_size = window_sizes[i]
    step_size = step_sizes[i]

    print(f'\n1. Generating Sliding Window DataFrame...')
    print(f'Window Size: {window_size}, Step Size: {step_size}')
    window_df = generate_sliding_window_dataframe(PredictionTask.TIME_TO_ERUPTION, sr, window_size, step_size)

    print(f'\n2. Extracting Features...')
    X, y = generate_X_y(PredictionTask.TIME_TO_ERUPTION, window_df)
    extracted_features = extract_all_features(X)

    print(f'\n3. Selecting Relevant Features...')
    relevant_features = select_relevant_features(extracted_features, y)

    print(f'\n4. Splitting Data...')
    X_train, X_test, y_train, y_test = train_test_split(relevant_features, y, test_size=0.2, random_state=42)

    print(f'\n5. Selecting Model...')
    report = select_model(PredictionTask.TIME_TO_ERUPTION, X_train, X_test, y_train, y_test)

    print(f'\n6. Report:')

report

"""# Machine Learning: Predict Eruption Sensor Reading"""

# Eruption Sensor Reading
window_sizes = [10, 50, 100, 300, 500]
step_sizes = [5, 25, 50, 100, 300]

for i in range(len(window_sizes)):
    window_size = window_sizes[i]
    step_size = step_sizes[i]


print(f'\n1. Generating Sliding Window DataFrame...')
print(f'Window Size: {window_size}, Step Size: {step_size}')
window_df = generate_sliding_window_dataframe(PredictionTask.ERUPTION_SENSOR_READING, sr, window_size, step_size)

print(f'\n2. Extracting Features...')
X, y = generate_X_y(PredictionTask.ERUPTION_SENSOR_READING, window_df)
extracted_features = extract_all_features(X)

print(f'\n3. Selecting Relevant Features...')
relevant_features = select_relevant_features(extracted_features, y)

print(f'\n4. Splitting Data...')
X_train, X_test, y_train, y_test = train_test_split(relevant_features, y, test_size=0.2, random_state=42)

print(f'\n5. Selecting Model...')
report = select_model(PredictionTask.ERUPTION_SENSOR_READING, X_train, X_test, y_train, y_test)

print(f'\n6. Report:')

report

"""# Hyperparameter Tuning

## ExtraTreesRegressor - best model during experimentation

Other good candidates:
- RandomForest Regressor
-

"""

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.ensemble import ExtraTreesRegressor

# Define the model and parameter grid
model = ExtraTreesRegressor(random_state=42)
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2']
}

# Define the grid search
grid = GridSearchCV(model, param_grid, cv=5, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')

# Fit the grid search
grid.fit(X_train, y_train)

# Get the best estimator
best_model = grid.best_estimator_

# Make predictions
y_pred = best_model.predict(X_test)

# Evaluate the best model on the test set
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error (MSE):", mse)
print("R-squared (R2):", r2)

# # merge v_param_df and selected_features columns
# selected_features = pd.concat([v_param_df, selected_features], axis=1)
# selected_features

"""# Additional Alternatives

## TPOT: Lazy Predict Alternative (For Automated ML)
"""

from sklearn.metrics import r2_score
from tpot import TPOTRegressor

# TPOT for automated machine learning
tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2, random_state=42)
tpot.fit(X_train, y_train)
tpot.score(X_test, y_test)
tpot.export('tpot_tilt_erupt_pipeline.py')
# Evaluate the best model on the test set
best_model = tpot.fitted_pipeline_
y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error (MSE):", mse)
print("R-squared (R2):", r2)

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# NOTE: Make sure that the outcome column is labeled 'target' in the data file
# tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)
# features = tpot_data.drop('target', axis=1)
# training_features, testing_features, training_target, testing_target = \
#     train_test_split(features, tpot_data['target'], random_state=42)

# Average CV score on the training set was: -15856.737621293922
exported_pipeline = RandomForestRegressor(bootstrap=True, max_features=0.05, min_samples_leaf=7, min_samples_split=13, n_estimators=100)
# Fix random state in exported estimator
if hasattr(exported_pipeline, 'random_state'):
    setattr(exported_pipeline, 'random_state', 42)

exported_pipeline.fit(X_train, y_train)
results = exported_pipeline.predict(X_test)